#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# noinspection SpellCheckingInspection
"""
@File    :   particle_filter.py
@Time    :   2021/11/10 22:01
@Author  :   Jinnan Huang
@Contact :   jinnan_huang@stu.xjtu.edu.cn
@Desc    :   None
"""
import logging
from collections import Counter
from functools import partial
from multiprocessing import Pool, cpu_count
from typing import List, Tuple

import numpy as np
from scipy.stats import gamma as scipy_gamma_dist

from IBHP_simulation import IBHP


# noinspection PyPep8Naming,PyShadowingNames
def transfer_multi_dist_result_to_vec(T_array: np.ndarray):
    """
    transform the data matrix generated by multinomial distribution to a concrete word matrix
    :param T_array: text vector
    :return:
    """

    def transfer_occurrence_data(idx: list, data: list):
        res = []
        res.extend([[idx[i]] * data[i] for i in range(len(idx))])
        res = [i for k in res for i in k]
        return res

    word_index = np.argwhere(T_array != 0)
    index_list = [word_index[word_index[:, 0] == n][:, 1].tolist() for n in np.unique(word_index[:, 0])]
    word_occurrence_list = [T_array[i, idx].tolist() for i, idx in enumerate(index_list)]
    word_corpus_mat = np.array([transfer_occurrence_data(index_list[i], word_occurrence_list[i])
                                for i in range(len(index_list))])
    return word_corpus_mat


# noinspection PyMissingConstructor,DuplicatedCode,PyPep8Naming,PyShadowingNames
class Particle(IBHP):
    # noinspection SpellCheckingInspection
    """
    This class implements all the steps of single particle sampling, hyperparameter updating and particle weight
    calculation.
    """

    # noinspection SpellCheckingInspection
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S",
                        level=logging.INFO)

    # noinspection PyShadowingNames
    def __init__(self, word_dict: np.ndarray, timestamp_array: np.ndarray, T_array: np.ndarray, L: int = 3):
        """
        :param T_array: text vector
        :param L: Number of base kernels
        :param word_dict: dictionary
        """
        super(Particle, self).__init__()
        self.log_particle_weight = None
        self.T_array = T_array
        self.timestamp_array = timestamp_array
        self.L = L
        self.word_dict = word_dict
        self.S = len(self.word_dict)

    def calculate_lambda_k(self, n):
        """
        calculate lambda_k array for each event
        :param n:
        :return:
        """
        if n == 1:
            self.lambda_k_array = self.w.T @ self.beta
        elif n >= 2:
            delta_t_array = self.timestamp_array[n - 1] - self.timestamp_array[: n]

            base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
            base_kernel_mat = base_kernel_for_delta_t_vec(delta_t_array, self.beta, self.tau).T  # t_i for each row
            kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c
            kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)
            self.lambda_k_array = np.sum(np.divide(kappa_history, kappa_history_count), axis=0)

    def sample_first_particle_event_status(self):
        """
        Generate the particle state corresponding to the first Event
        :return:
        """
        # Initialize lambda0, beta, tau
        self.lambda0 = 1
        self.beta = np.array([2, 2, 2])  # array, shape=(L, )
        self.tau = np.array([0.2, 0.2, 0.2])  # array, shape=(L, )

        self.w_0 = np.array([1 / self.L] * self.L)  # array, shape=(L, )
        self.v_0 = np.array([1] * self.S)  # array, shape(S, )

        # Generate K
        self.K = np.random.poisson(self.lambda0, 1)[0]  # K: int
        while self.K == 0:
            logging.info(f'[event 1] Generated topic number K is 0, regenerate')
            self.K = np.random.poisson(self.lambda0, 1)[0]

        # Generate the topic occurrence matrix c, set all c_k=1
        self.c = np.ones((1, self.K))  # matrix, shape=(1, self.K)
        # Generate w
        self.w = np.random.dirichlet(self.w_0, self.K).T  # matrix, shape=(L, K), each column is the weight of each k
        # Generate v
        self.v = np.random.dirichlet(self.v_0, self.K).T  # matrix, shape=(S, K), Each column is a distribution of
        # words for each k

        # calculate kappa_n
        self.kappa_n = self.w.T @ self.beta

        # calculate lambda_1
        self.calculate_lambda_k(n=1)
        self.lambda_tn_array = np.array([np.sum(self.lambda_k_array) + self.lambda0])

    def sample_particle_following_event_status(self, n: int):
        """
        Generate the particle state corresponding to the following event
        :param n: n-th event
        :return:
        """
        # Calculate the probability of generating c from the existing K topics
        p = self.lambda_k_array / (self.lambda0 / self.K + self.lambda_k_array)
        # Generate topic occurrence vectors with K topics
        generate_old_c_func = np.vectorize(self.generate_c)
        c_old = generate_old_c_func(p)
        # calculate delta_t_vec, tn-ti
        delta_t_vec = self.timestamp_array[n - 1] - self.timestamp_array[: n]

        # Generate K+
        k_plus = np.random.poisson(self.lambda0 / (self.lambda0 + np.sum(self.lambda_k_array)), 1)[0]

        # When K+ is equal to 0, check whether c_old is all 0, and if it is all 0, regenerate c_old
        if k_plus == 0:
            while np.all(c_old == 0):
                logging.info(f'[event {n}]When K+ is 0, c_old is also all 0, and c_old is regenerated')
                c_old = generate_old_c_func(p)
        # update K
        self.K = self.K + k_plus

        if k_plus:
            # If K+ is greater than 0, initialize a new topic occurrence vector
            c_new = np.ones(k_plus)
            c = np.hstack((c_old, c_new))
            self.c = np.hstack((self.c, np.zeros((self.c.shape[0], k_plus))))  # Complete the existing c matrix with 0
            self.c = np.vstack((self.c, c))  # Add the new c vector to the c matrix

            # If K+ is greater than 0, generate a new w_k, update self.w
            w_new = np.random.dirichlet(self.w_0, k_plus).T
            self.w = np.hstack((self.w, w_new))

            # If K+ is greater than 0, generate a new v_k, update self.v
            v_new = np.random.dirichlet(self.v_0, k_plus).T
            self.v = np.hstack((self.v, v_new))

            # If K+ is greater than 0, calculate a new kappa_n
            new_kappa = w_new.T @ self.beta
            self.kappa_n = self.w[:, : c_old.shape[0]].T @ self.beta * c_old
            self.kappa_n = np.hstack((self.kappa_n, new_kappa))
        else:
            self.c = np.vstack((self.c, c_old))
            self.kappa_n = self.w.T @ self.beta * c_old

        # calculate lambda_tn
        kappa_n_nonzero_index = np.argwhere(self.kappa_n != 0)[:, 0]
        self.lambda_tn_array = np.append(self.lambda_tn_array,
                                         np.sum(self.lambda_k_array[kappa_n_nonzero_index]) + self.lambda0)

    # noinspection SpellCheckingInspection
    def log_hawkes_likelihood(self, n, lambda0, beta, tau):
        """
        log hawkes likelihood
        :param tau: candidate tau
        :param beta: candidate beta
        :param lambda0: candidate lambda0
        :param n: The current sample order, which must be greater than or equal to 1
        :return:
        """
        delta_T_array = self.timestamp_array[-1] - self.timestamp_array[: n]  # used to calculate the integral term
        delta_tn_array = self.timestamp_array[n - 1] - self.timestamp_array[: n]  # for calculating kappa i

        # ---------------------- first term ----------------------
        # calculate kappa history
        base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
        base_kernel_mat = base_kernel_for_delta_t_vec(delta_tn_array, beta, tau).T
        kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c
        kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)

        # calculate exp_term in integral term
        delta_t_divide_tau = np.vectorize(np.divide, signature='(),(l)->(l)')
        exp_term = 1 - np.exp(- delta_t_divide_tau(delta_T_array, tau))  # (t, l)

        # calculate exp_term_coefficient
        exp_term_coefficient_nominator = np.einsum('lk,l->lk', self.w, beta * tau)
        exp_times_coef_term = np.einsum('lk,tl->tk', exp_term_coefficient_nominator, exp_term) * self.c
        exp_times_coef_term = exp_times_coef_term / kappa_history_count
        kappa_n_nonzero_index = np.argwhere(kappa_history[-1] != 0)[:, 0]

        # log integral term
        integral_term = np.sum(exp_times_coef_term[:, kappa_n_nonzero_index])
        log_integral_term = - (lambda0 * self.timestamp_array[-1] + integral_term)

        # ---------------------- second term ----------------------
        log_prod_term = 0
        for j in np.arange(1, n + 1):
            delta_tj_array = self.timestamp_array[j - 1] - self.timestamp_array[: j]
            tj_kernel_mat = base_kernel_for_delta_t_vec(delta_tj_array, beta, tau).T
            kappa_history_j = np.einsum('lk,tl->tk', self.w, tj_kernel_mat) * self.c[: j]
            kappa_history_j_count = np.count_nonzero(kappa_history_j, axis=1).reshape(-1, 1)
            kappa_j_nonzero_index = np.argwhere(kappa_history_j[-1] != 0)[:, 0]
            lambda_tj = np.sum(np.divide(kappa_history_j, kappa_history_j_count)[:, kappa_j_nonzero_index])
            log_prod_term += np.log(lambda0 + lambda_tj)

        # log hawkes likelihood
        log_hawkes_likelihood = log_integral_term + log_prod_term
        return log_hawkes_likelihood

    @staticmethod
    def gamma_dist(shape, scale, x):
        """
        An expression for the gamma distribution,
        which is subsequently used to compute p-values for the proposed and prior distributions
        :param shape: parameter
        :param scale: parameter
        :param x: variable
        :return:
        """
        return scipy_gamma_dist.pdf(x=x, a=shape, scale=scale)

    # noinspection PyUnboundLocalVariable,SpellCheckingInspection
    def update_all_hyperparameters(self, shape_lambda0, scale_lambda0, shape_beta, scale_beta,
                                   shape_tau, scale_tau, n, N: int = 10000):
        """
        update lambda0, beta, tau
        :param N: sample numbers
        :param shape_lambda0: gamma prior parameter for lambda0
        :param scale_lambda0: gamma prior parameter for lambda0
        :param shape_beta: gamma prior parameter for beta
        :param scale_beta: gamma prior parameter for beta
        :param shape_tau: gamma prior parameter for tau
        :param scale_tau: gamma prior parameter for tau
        :param n: event number
        :return:
        """
        # draw candidate lambda0
        lambda0_candi_arr = np.random.gamma(shape_lambda0, scale_lambda0, N)
        # vectorize func
        lambda0_prior = np.vectorize(partial(self.gamma_dist, shape_lambda0, scale_lambda0))
        # calculate prior for candidate lambda0
        lambda0_p_prior_arr = lambda0_prior(lambda0_candi_arr)

        # draw candidate beta
        beta_candi_mat = np.random.gamma(shape_beta, scale_beta, (N, self.L))
        # vectorize func
        beta_prior = np.vectorize(partial(self.gamma_dist, shape_beta, scale_beta))
        # calculate prior for candidate beta
        beta_p_prior_mat = beta_prior(beta_candi_mat)

        # draw candidate tau
        tau_candi_mat = np.random.gamma(shape_tau, scale_tau, (N, self.L))
        # vectorize func
        tau_prior = np.vectorize(partial(self.gamma_dist, shape_tau, scale_tau))
        # calculate prior for candidate tau
        tau_p_prior_mat = tau_prior(tau_candi_mat)

        # calculate log-likelihood
        log_hawkes_likelihood_func = np.vectorize(partial(self.log_hawkes_likelihood, n), signature='(),(n),(n)->()')
        log_hawkes_likelihood_arr = log_hawkes_likelihood_func(lambda0_candi_arr, beta_candi_mat, tau_candi_mat)

        # normalize log-likelihood
        log_hawkes_likelihood_arr = np.exp(log_hawkes_likelihood_arr - np.max(log_hawkes_likelihood_arr))
        log_hawkes_likelihood_arr = log_hawkes_likelihood_arr / np.sum(log_hawkes_likelihood_arr)

        # calculate sample weight
        weight_arr = log_hawkes_likelihood_arr * lambda0_p_prior_arr * np.prod(beta_p_prior_mat, axis=1) * \
                     np.prod(tau_p_prior_mat, axis=1)
        # normalize weight
        weight_arr = weight_arr / np.sum(weight_arr)
        print(f'weight_arr: {weight_arr}')

        # new hyperparameters
        self.lambda0 = weight_arr @ lambda0_candi_arr
        self.beta = weight_arr @ beta_candi_mat
        self.tau = weight_arr @ tau_candi_mat

    # noinspection SpellCheckingInspection
    def update_log_particle_weight(self, old_particle_weight, n: int):
        """
        calculate and update log particle weight
        :param old_particle_weight: former event's particle weight
        :param n: The current sample number (sample order), should be greater than or equal to 1
        :return:
        """
        # calculate lambda_prime
        if n == 1:
            lambda_prime = self.lambda0 * self.timestamp_array[n - 1]
        else:
            lambda_prime = self.lambda0 * (self.timestamp_array[n - 1] - self.timestamp_array[n - 2])
            # delta_tn_array = self.timestamp_array[n - 1] - self.timestamp_array[: n - 1]
            # delta_tn_minus_1_array = self.timestamp_array[n - 2] - self.timestamp_array[: n - 1]
            #
            # # calculate kappa history
            # base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
            # base_kernel_mat = base_kernel_for_delta_t_vec(delta_tn_array, self.beta, self.tau).T
            # kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c[: n - 1]
            # kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)
            # kappa_n_minus_1_nonzero_index = np.argwhere(kappa_history[-1] != 0)[:, 0]
            #
            # # calculate sharing coefficient
            # exp_term_coefficient_nominator = np.einsum('lk,l->lk', self.w, self.beta * self.tau)
            #
            # # calculate first exp term: tn-ti
            # delta_t_divide_tau = np.vectorize(np.divide, signature='(),(l)->(l)')
            # exp_term_tn = np.exp(delta_t_divide_tau(delta_tn_array, self.tau))
            # exp_times_coef_term_tn = np.einsum('lk,tl->tk', exp_term_coefficient_nominator, exp_term_tn) * self.c[
            #                                                                                                : n - 1]
            # exp_times_coef_term_tn = exp_times_coef_term_tn / kappa_history_count
            # first_term = np.sum(exp_times_coef_term_tn[:, kappa_n_minus_1_nonzero_index])
            #
            # # calculate second exp term: t_n-1 - ti
            # exp_term_tn_1 = np.exp((delta_t_divide_tau(delta_tn_minus_1_array, self.tau)))
            # exp_times_coef_term_tn_minus_1 = np.einsum('lk,tl->tk', exp_term_coefficient_nominator,
            #                                            exp_term_tn_1) * self.c[
            #                                                             : n - 1]
            # exp_times_coef_term_tn_minus_1 = exp_times_coef_term_tn_minus_1 / kappa_history_count
            # second_term = np.sum(exp_times_coef_term_tn_minus_1[:, kappa_n_minus_1_nonzero_index])
            #
            # # calculate lambda_prime
            # lambda_prime = self.lambda0 * (self.timestamp_array[n - 1] - self.timestamp_array[n - 2]) - \
            #                (first_term - second_term)
        print(f'[event {n}] lambda_prime: {lambda_prime}')
        # calculate log likelihood for timestamp
        log_likelihood_timestamp = np.log(lambda_prime) - lambda_prime

        # log likelihood for text
        kappa_n_minus_1_nonzero_index = np.argwhere(self.kappa_n != 0)[:, 0]
        vn_avg = np.einsum('ij->i', self.v[:, kappa_n_minus_1_nonzero_index]) / np.count_nonzero(self.kappa_n)
        Tn = transfer_multi_dist_result_to_vec(self.T_array)[n - 1]  # shape=(S, )
        count_dict = Counter(Tn)
        log_likelihood_text = 0
        for k, v in count_dict.items():
            log_likelihood_text += v * np.log(vn_avg[k])
        # Calculate the updated log particle weight
        self.log_particle_weight = np.log(old_particle_weight) + log_likelihood_timestamp + log_likelihood_text


# noinspection PyPep8Naming,SpellCheckingInspection,PyShadowingNames
class Particle_Filter:
    """
    This class controls weight updating, normalization, and resampling of all particles (in parallel)
    """

    def __init__(self, timestamp_array: np.ndarray, T_array: np.ndarray,
                 n_particle: int, word_dict: np.ndarray, L: int = 3):
        """
        Generates particles and initializes particle weights

        :param timestamp_array: timestamp vector
        :param T_array: text
        :param n_particle: number of particles
        :param word_dict: dictionary
        :param L: The number of base kernels
        """
        assert len(timestamp_array) == T_array.shape[0]
        self.n_sample = T_array.shape[0]
        self.n_particle = n_particle
        self.word_corpus = word_dict
        self.particle_list = [Particle(word_dict=word_dict, timestamp_array=timestamp_array, T_array=T_array, L=L) for
                              i in np.arange(self.n_particle)]
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)

    def get_particle_list(self):
        return self.particle_list

    def get_particle_num(self):
        return self.n_particle

    def get_partcie_weight_arr(self):
        return self.particle_weight_arr

    def normalize_particle_weight(self):
        """
        Normalize particle weights (map weights to 0-1 interval and sum to 1)
        :return:
        """
        self.particle_weight_arr = np.exp(self.particle_weight_arr - np.max(self.particle_weight_arr))
        self.particle_weight_arr = self.particle_weight_arr / np.sum(self.particle_weight_arr)
        logging.info(f'particle weight: {self.particle_weight_arr}')

    def resample_particles(self):
        """
        Resampling particles, resampling n_particle particles
        :return:
        """
        new_particle_list = []
        sorted_particle_index = np.argsort(self.particle_weight_arr)  # 得到的是粒子权重升序排列的索引
        sorted_particle_weight = self.particle_weight_arr[sorted_particle_index]  # 升序排列后的粒子权重
        # 构造粒子对应的权重区间
        for i in np.arange(self.n_particle - 1, -1, -1):
            sorted_particle_weight[i] = np.sum(sorted_particle_weight[: i + 1])
        # 重新采样n_particle个粒子
        for i in np.arange(self.n_particle):
            u = np.random.rand()
            nearest_elem_idx = np.argmin(np.abs(u - sorted_particle_weight))
            if u <= sorted_particle_weight[nearest_elem_idx]:
                new_particle = self.particle_list[nearest_elem_idx]
            else:
                new_particle = self.particle_list[nearest_elem_idx + 1]
            new_particle_list.append(new_particle)
        # 更新粒子列表，重新设置粒子权重
        self.particle_list = new_particle_list
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)

    def generate_first_event_status_for_each_particle(self, parameter_num: int,
                                                      particle_idx_pair: Tuple[int, Particle]):
        """
        Generate the state corresponding to the first event for each particle
        :param parameter_num: Number of parameter samples to be sampled
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        logging.info(f'[event 1, paricle {particle_idx + 1}] Sampling particle status……')
        particle.sample_first_particle_event_status()
        # Update hyperparameters and triggering kernels
        logging.info(f'[event 1, paricle {particle_idx + 1}] Updating hyperparameters and trigger kernel……')
        particle.update_all_hyperparameters(N=parameter_num,
                                            shape_lambda0=1, scale_lambda0=1,
                                            shape_beta=1, scale_beta=1,
                                            shape_tau=1, scale_tau=1, n=1)

        # Calculate and update the log particle weights
        logging.info(f'[event 1, paricle {particle_idx + 1}] Updating particle weight……')
        particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=1)
        return particle_idx, particle

    def generate_following_event_status_for_each_paritcle(self, n: int, parameter_num: int,
                                                          particle_idx_pair: Tuple[int, Particle]):
        """

        :param parameter_num: Number of parameter samples to be sampled
        :param n:
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        logging.info(f'[event {n}, particle {particle_idx + 1}] Sampling particle status……')
        particle.sample_particle_following_event_status(n)
        logging.info(f'[event {n}, particle {particle_idx + 1}] Updating hyperparameters and trigger kernel……')
        particle.update_all_hyperparameters(N=parameter_num,
                                            shape_lambda0=1, scale_lambda0=1,
                                            shape_beta=1, scale_beta=1,
                                            shape_tau=1, scale_tau=1,
                                            n=n)
        logging.info(f'[event {n}, particle {particle_idx + 1}] Updating particle weight……')
        particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=n)
        return particle_idx, particle

    def update_particle_weight_arr(self, particle_index_list: List[Tuple[int, Particle]]):
        """
        Update particle weight list
        :param particle_index_list:
        :return:
        """
        for idx, particle in particle_index_list:
            self.particle_weight_arr[idx] = particle.log_particle_weight


# noinspection SpellCheckingInspection
if __name__ == '__main__':
    # Generate test data
    # parameters
    # n_sample = int(sys.argv[1])
    # n_particle = int(sys.argv[2])
    # parameter_sample_num = int(sys.argv[3])

    n_sample = 1000
    n_particle = 100
    parameter_sample_num = 20000

    logging.info(f'set n_sample to {n_sample}')
    logging.info(f'set n_particle to {n_particle}')
    logging.info(f'set N to {parameter_sample_num}')

    # noinspection SpellCheckingInspection
    ibhp = IBHP(n_sample)
    ibhp.generate_data()
    logging.info(f'\n{"-" * 40} Observational data generated {"-" * 40}\n')
    logging.info(f'Timestamp: {ibhp.timestamp_array}\n')
    logging.info(f'Text: {transfer_multi_dist_result_to_vec(ibhp.text)}\n')
    word_dict = np.arange(1000)
    logging.info(f'Dictionary: {word_dict}\n')

    # filtering
    # noinspection PyBroadException,SpellCheckingInspection
    logging.info(f'\n{"-" * 40}  Start particle filter parameter estimation {"-" * 40}\n')
    pf = Particle_Filter(timestamp_array=ibhp.timestamp_array, T_array=ibhp.text, n_particle=n_particle,
                         word_dict=word_dict, L=3)
    particle_index_pair_list = [(idx, particle) for idx, particle in enumerate(pf.get_particle_list())]
    # event 1 status
    pool_event_1 = Pool(cpu_count())
    particle_index_pair_list = list(pool_event_1.map(partial(pf.generate_first_event_status_for_each_particle,
                                                             parameter_sample_num),
                                                     particle_index_pair_list))
    pool_event_1.close()
    pool_event_1.join()
    pf.update_particle_weight_arr(particle_index_list=particle_index_pair_list)
    pf.normalize_particle_weight()
    logging.info(f'[event 1] Normalized particle weight: \n{pf.get_partcie_weight_arr()}')
    N_eff = 1 / np.sum(np.square(pf.get_partcie_weight_arr()))
    if N_eff < 2 / 3 * pf.get_particle_num():
        logging.info(f'[event 1] Resampling particles……')
        pf.resample_particles()

    # event 2~n status
    # noinspection SpellCheckingInspection
    for n in np.arange(2, n_sample + 1):
        # process pool for event n
        pool_event_n = Pool(cpu_count())
        particle_index_pair_list = list(
            pool_event_n.map(partial(pf.generate_following_event_status_for_each_paritcle, n, parameter_sample_num),
                             particle_index_pair_list))
        pool_event_n.close()
        pool_event_n.join()
        pf.update_particle_weight_arr(particle_index_list=particle_index_pair_list)
        pf.normalize_particle_weight()

        # output
        logging.info(f'[event {n}] Normalized particle weight: {pf.get_partcie_weight_arr()}')
        # particle weight
        p_weight_arr = pf.get_partcie_weight_arr()
        logging.info(f'weight of all particles: {p_weight_arr}\n')

        # Hyperparameters weighted average
        lam_0_arr = np.array([particle.lambda0 for idx, particle in particle_index_pair_list])
        lam_0 = np.average(lam_0_arr, weights=p_weight_arr)
        beta_arr = np.array([particle.beta.reshape(-1) for idx, particle in particle_index_pair_list])
        beta = np.average(beta_arr, axis=0, weights=p_weight_arr)
        tau_arr = np.array([particle.tau.reshape(-1) for idx, particle in particle_index_pair_list])
        tau = np.average(tau_arr, axis=0, weights=p_weight_arr)
        logging.info(f'weighted average of three hyperparameters: \n lambda_0: {lam_0}\n beta: {beta}\n tau: {tau}\n')

        if n == n_sample:
            break
        N_eff = 1 / np.sum(np.square(pf.get_partcie_weight_arr()))
        if N_eff < 2 / 3 * pf.get_particle_num():
            logging.info(f'[event {n}] Resampling particles……')
            pf.resample_particles()
