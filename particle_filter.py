#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# noinspection SpellCheckingInspection
"""
@File    :   particle_filter.py
@Time    :   2021/11/10 22:01
@Author  :   Jinnan Huang
@Contact :   jinnan_huang@stu.xjtu.edu.cn
@Desc    :   None
"""
import logging
from collections import Counter
from copy import deepcopy
from functools import partial
from multiprocessing import Pool, cpu_count
from typing import List, Tuple

import numpy as np
import scipy.stats as sta
from tqdm import tqdm

from IBHP_simulation import IBHP


# noinspection PyPep8Naming,PyShadowingNames
def transfer_multi_dist_result_to_vec(T_array: np.ndarray):
    """
    transform the data matrix generated by multinomial distribution to a concrete word matrix
    :param T_array: text vector
    :return:
    """

    def transfer_occurrence_data(idx: list, data: list):
        res = []
        res.extend([[idx[i]] * data[i] for i in range(len(idx))])
        res = [i for k in res for i in k]
        return res

    word_index = np.argwhere(T_array != 0)
    index_list = [word_index[word_index[:, 0] == n][:, 1].tolist() for n in np.unique(word_index[:, 0])]
    word_occurrence_list = [T_array[i, idx].tolist() for i, idx in enumerate(index_list)]
    word_corpus_mat = np.array([transfer_occurrence_data(index_list[i], word_occurrence_list[i])
                                for i in range(len(index_list))])
    return word_corpus_mat


# noinspection PyMissingConstructor,DuplicatedCode,PyPep8Naming,PyShadowingNames
class Particle(IBHP):
    # noinspection SpellCheckingInspection
    """
    This class implements all the steps of single particle sampling, hyperparameter updating and particle weight
    calculation.
    """

    # noinspection SpellCheckingInspection
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S",
                        level=logging.INFO)

    # noinspection PyShadowingNames
    def __init__(self, word_dict: np.ndarray, timestamp_array: np.ndarray, T_array: np.ndarray, L: int = 3):
        """
        :param T_array: text vector
        :param L: Number of base kernels
        :param word_dict: dictionary
        """
        super(Particle, self).__init__()
        self.log_particle_weight = None
        self.T_array = T_array
        self.timestamp_array = timestamp_array
        self.L = L
        self.word_dict = word_dict
        self.S = len(self.word_dict)

    def calculate_lambda_k(self, n):
        """
        calculate lambda_k array for each event
        :param n:
        :return:
        """
        if n == 1:
            self.lambda_k_array = self.w.T @ self.beta
        elif n >= 2:
            delta_t_array = self.timestamp_array[n - 1] - self.timestamp_array[: n]

            base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
            base_kernel_mat = base_kernel_for_delta_t_vec(delta_t_array, self.beta, self.tau).T  # t_i for each row
            kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c
            kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)
            self.lambda_k_array = np.sum(np.divide(kappa_history, kappa_history_count), axis=0)

    def sample_first_particle_event_status(self):
        """
        Generate the particle state corresponding to the first Event
        :return:
        """
        # Initialize lambda0, beta, tau
        self.lambda0 = 1
        self.beta = np.array([2, 2, 2])  # array, shape=(L, )
        self.tau = np.array([0.2, 0.2, 0.2])  # array, shape=(L, )

        self.w_0 = np.array([1 / self.L] * self.L)  # array, shape=(L, )
        self.v_0 = np.array([1] * self.S)  # array, shape(S, )

        # Generate K
        self.K = np.random.poisson(self.lambda0, 1)[0]  # K: int
        while self.K == 0:
            logging.info(f'[event 1] Generated topic number K is 0, regenerate')
            self.K = np.random.poisson(self.lambda0, 1)[0]

        # Generate the topic occurrence matrix c, set all c_k=1
        self.c = np.ones((1, self.K))  # matrix, shape=(1, self.K)
        # Generate w
        self.w = np.random.dirichlet(self.w_0, self.K).T  # matrix, shape=(L, K), each column is the weight of each k
        # Generate v
        self.v = np.random.dirichlet(self.v_0, self.K).T  # matrix, shape=(S, K), Each column is a distribution of
        # words for each k

        # calculate kappa_n
        self.kappa_n = self.w.T @ self.beta

        # calculate lambda_1
        self.calculate_lambda_k(n=1)
        self.lambda_tn_array = np.array([np.sum(self.lambda_k_array)])

    def sample_particle_following_event_status(self, n: int):
        """
        Generate the particle state corresponding to the following event
        :param n: n-th event
        :return:
        """
        # Calculate the probability of generating c from the existing K topics
        p = self.lambda_k_array / (self.lambda0 / self.K + self.lambda_k_array)
        # Generate topic occurrence vectors with K topics
        generate_old_c_func = np.vectorize(self.generate_c)
        c_old = generate_old_c_func(p)
        # calculate delta_t_vec, tn-ti
        delta_t_vec = self.timestamp_array[n - 1] - self.timestamp_array[: n]

        # Generate K+
        k_plus = np.random.poisson(self.lambda0 / (self.lambda0 + np.sum(self.lambda_k_array)), 1)[0]

        # When K+ is equal to 0, check whether c_old is all 0, and if it is all 0, regenerate c_old
        if k_plus == 0:
            while np.all(c_old == 0):
                logging.info(f'[event {n}]When K+ is 0, c_old is also all 0, and c_old is regenerated')
                c_old = generate_old_c_func(p)
        # update K
        self.K = self.K + k_plus

        if k_plus:
            # If K+ is greater than 0, initialize a new topic occurrence vector
            c_new = np.ones(k_plus)
            c = np.hstack((c_old, c_new))
            self.c = np.hstack((self.c, np.zeros((self.c.shape[0], k_plus))))  # Complete the existing c matrix with 0
            self.c = np.vstack((self.c, c))  # Add the new c vector to the c matrix

            # If K+ is greater than 0, generate a new w_k, update self.w
            w_new = np.random.dirichlet(self.w_0, k_plus).T
            self.w = np.hstack((self.w, w_new))

            # If K+ is greater than 0, generate a new v_k, update self.v
            v_new = np.random.dirichlet(self.v_0, k_plus).T
            self.v = np.hstack((self.v, v_new))

            # If K+ is greater than 0, calculate a new kappa_n
            new_kappa = w_new.T @ self.beta
            self.kappa_n = self.w[:, : c_old.shape[0]].T @ self.beta * c_old
            self.kappa_n = np.hstack((self.kappa_n, new_kappa))
        else:
            self.c = np.vstack((self.c, c_old))
            self.kappa_n = self.w.T @ self.beta * c_old

        # calculate lambda_tn
        kappa_n_nonzero_index = np.argwhere(self.kappa_n != 0)[:, 0]
        self.lambda_tn_array = np.append(self.lambda_tn_array,
                                         np.sum(self.lambda_k_array[kappa_n_nonzero_index]))

    # noinspection SpellCheckingInspection
    def log_hawkes_likelihood(self, n, lambda0, beta, tau):
        """
        log hawkes likelihood
        :param tau: candidate tau
        :param beta: candidate beta
        :param lambda0: candidate lambda0
        :param n: The current sample order, which must be greater than or equal to 1
        :return:
        """
        delta_T_array = self.timestamp_array[-1] - self.timestamp_array[: n]  # used to calculate the integral term
        delta_tn_array = self.timestamp_array[n - 1] - self.timestamp_array[: n]  # for calculating kappa i

        # ---------------------- first term ----------------------
        # calculate kappa history
        base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
        base_kernel_mat = base_kernel_for_delta_t_vec(delta_tn_array, beta, tau).T
        kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c
        kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)
        kappa_history_count[kappa_history_count == 0] = 1  # Make sure the denominator is not 0

        # calculate exp_term in integral term
        delta_t_divide_tau = np.vectorize(np.divide, signature='(),(l)->(l)')
        exp_term = 1 - np.exp(- delta_t_divide_tau(delta_T_array, tau))  # (t, l)

        # calculate exp_term_coefficient
        exp_term_coefficient_nominator = np.einsum('lk,l->lk', self.w, beta * tau)
        exp_times_coef_term = np.einsum('lk,tl->tk', exp_term_coefficient_nominator, exp_term) * self.c
        exp_times_coef_term = exp_times_coef_term / kappa_history_count
        kappa_n_nonzero_index = np.argwhere(kappa_history[-1] != 0)[:, 0]

        # log integral term
        integral_term = np.sum(exp_times_coef_term[:, kappa_n_nonzero_index])
        log_integral_term = - (lambda0 * self.timestamp_array[-1] + integral_term)

        # ---------------------- second term ----------------------
        log_prod_term = 0
        for j in np.arange(1, n + 1):
            delta_tj_array = self.timestamp_array[j - 1] - self.timestamp_array[: j]
            tj_kernel_mat = base_kernel_for_delta_t_vec(delta_tj_array, beta, tau).T
            kappa_history_j = np.einsum('lk,tl->tk', self.w, tj_kernel_mat) * self.c[: j]
            kappa_history_j_count = np.count_nonzero(kappa_history_j, axis=1).reshape(-1, 1)
            kappa_history_j_count[kappa_history_j_count == 0] = 1  # Make sure the denominator is not 0
            kappa_j_nonzero_index = np.argwhere(kappa_history_j[-1] != 0)[:, 0]
            lambda_tj = np.sum(np.divide(kappa_history_j, kappa_history_j_count)[:, kappa_j_nonzero_index])
            log_prod_term += np.log(lambda0 + lambda_tj)

        # log hawkes likelihood
        log_hawkes_likelihood = log_integral_term + log_prod_term
        return log_hawkes_likelihood

    def update_lambda0(self, n: int, n_mh: int, beta, tau, old_lambda0=None):
        """
        update lambda0 using Metropolis-Hastings Algorithm
        :param old_lambda0:
        :param tau: updated tau
        :param beta: update beta
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :return:
        """
        if n_mh == 1:
            lambda0_old = self.lambda0
        else:
            lambda0_old = old_lambda0
        lambda0_candidate = sta.gamma.rvs(
            a=lambda0_old)  # draw candidate lambda0 from proposal distribution(related to previous lambda0)
        lambda0_old_prior = sta.gamma.pdf(x=lambda0_old, a=3)  # probability of lambda0_old proposal distribution
        lambda0_candidate_prior = sta.gamma.pdf(x=lambda0_candidate,
                                                a=3)  # probability of lambda0_candidate proposal distribution

        # likelihood
        log_hawkes_likelihood_lambda0_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0_old,
                                                                       beta=beta, tau=tau)
        log_hawkes_likelihood_lambda0_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0_candidate,
                                                                             beta=beta, tau=tau)

        # probability of proposal distribution
        lambda0_candidate_proposal = sta.gamma.pdf(x=lambda0_old, a=lambda0_candidate)
        lambda0_old_proposal = sta.gamma.pdf(x=lambda0_candidate, a=lambda0_old)

        log_accept_ratio = np.log(lambda0_candidate_prior) + log_hawkes_likelihood_lambda0_candidate + np.log(
            lambda0_candidate_proposal) - np.log(lambda0_old_prior) - log_hawkes_likelihood_lambda0_old - np.log(
            lambda0_old_proposal)
        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return lambda0_candidate
        else:
            return lambda0_old

    def update_beta(self, n, n_mh, index, lambda0, beta, tau, old_beta_l=None):
        """
        update each beta using Metropolis-Hastings Algorithm
        :param beta:
        :param tau: updated tau
        :param lambda0: updated lambda0
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :param index: the index of beta in beta array
        :param old_beta_l: the index of old beta in beta array
        :return:
        """

        def copy_beta(replace_index, replace_value, input_beta):
            """
            copy self.beta, replace specific value
            :param input_beta:
            :param replace_index:
            :param replace_value:
            :return:
            """
            beta = deepcopy(input_beta)
            beta[replace_index] = replace_value
            return beta

        if n_mh == 1:
            beta_l_old = self.beta[index]
        else:
            beta_l_old = old_beta_l

        # candidate rvs
        beta_l_candidate = sta.gamma.rvs(a=beta_l_old)

        # prior
        beta_l_old_prior = sta.gamma.pdf(x=beta_l_old, a=3)
        beta_l_candidate_prior = sta.gamma.pdf(x=beta_l_candidate, a=3)

        # likelihood
        beta_old = copy_beta(index, beta_l_old, beta)
        beta_candidate = copy_beta(index, beta_l_candidate, beta)
        log_hawkes_likelihood_beta_l_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0, beta=beta_old,
                                                                      tau=tau)
        log_hawkes_likelihood_beta_l_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0,
                                                                            beta=beta_candidate, tau=tau)

        # proposal
        beta_l_candidate_proposal = sta.gamma.pdf(x=beta_l_old, a=beta_l_candidate)
        beta_l_old_proposal = sta.gamma.pdf(x=beta_l_candidate, a=beta_l_old)

        log_accept_ratio = np.log(beta_l_candidate_prior) + log_hawkes_likelihood_beta_l_candidate + np.log(
            beta_l_candidate_proposal) - np.log(beta_l_old_prior) - log_hawkes_likelihood_beta_l_old - np.log(
            beta_l_old_proposal)

        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return beta_l_candidate
        else:
            return beta_l_old

    def update_tau(self, n, n_mh, index, lambda0, beta, tau, old_tau_l=None):
        """
        update each tau using Metropolis-Hastings Algorithm
        :param tau:
        :param beta:
        :param lambda0:
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :param index: the index of tau in tau array
        :param old_tau_l: the index of old tau in tau array
        :return:
        """

        def copy_tau(replace_index, replace_value, input_tau):
            """
            copy self.tau, replace specific value
            :param input_tau:
            :param replace_index:
            :param replace_value:
            :return:
            """
            tau = deepcopy(input_tau)
            tau[replace_index] = replace_value
            return tau

        if n_mh == 1:
            tau_l_old = self.tau[index]
        else:
            tau_l_old = old_tau_l

        # candidate rvs
        tau_l_candidate = sta.gamma.rvs(a=tau_l_old)

        # prior
        tau_l_old_prior = sta.gamma.pdf(x=tau_l_old, a=1)
        tau_l_candidate_prior = sta.gamma.pdf(x=tau_l_candidate, a=1)

        # likelihood
        tau_old = copy_tau(index, tau_l_old, tau)
        tau_candidate = copy_tau(index, tau_l_candidate, tau)
        log_hawkes_likelihood_tau_l_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0, beta=beta,
                                                                     tau=tau_old)
        log_hawkes_likelihood_tau_l_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0,
                                                                           beta=beta, tau=tau_candidate)

        # proposal
        tau_l_candidate_proposal = sta.gamma.pdf(x=tau_l_old, a=tau_l_candidate)
        tau_l_old_proposal = sta.gamma.pdf(x=tau_l_candidate, a=tau_l_old)

        log_accept_ratio = np.log(tau_l_candidate_prior) + log_hawkes_likelihood_tau_l_candidate + np.log(
            tau_l_candidate_proposal) - np.log(tau_l_old_prior) - log_hawkes_likelihood_tau_l_old - np.log(
            tau_l_old_proposal)

        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return tau_l_candidate
        else:
            return tau_l_old

    def mh_update(self, n, n_iter=100000):
        """
        update parameters
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_iter: number of iterations
        :return:
        """
        beta = deepcopy(self.beta)
        tau = deepcopy(self.tau)
        updated_lambda0, updated_beta_l, updated_tau_l = None, None, None
        updated_lambda0_array = None
        updated_beta_list = [[] for i in np.arange(self.L)]
        updated_tau_list = [[] for i in np.arange(self.L)]

        mh_iter = tqdm(np.arange(n_iter))

        for i in mh_iter:
            mh_iter.set_description(f'[event {n}] sampling parameters through MH')
            if i == 0:
                updated_lambda0 = self.update_lambda0(n=n, n_mh=i + 1, beta=beta, tau=tau)
                updated_lambda0_array = np.array([updated_lambda0])
                for idx in np.arange(self.L):
                    updated_beta_l = self.update_beta(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                      tau=tau)
                    updated_beta_list[idx].append(updated_beta_l)
                    beta[idx] = updated_beta_l  # update beta_l after sampling
                    updated_tau_l = self.update_tau(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                    tau=tau)
                    updated_tau_list[idx].append(updated_tau_l)
                    tau[idx] = updated_tau_l
            else:
                updated_lambda0 = self.update_lambda0(n=n, n_mh=i + 1, beta=beta, tau=tau, old_lambda0=updated_lambda0)
                updated_lambda0_array = np.append(updated_lambda0_array, updated_lambda0)
                for idx in np.arange(self.L):
                    updated_beta_l = self.update_beta(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                      tau=tau,
                                                      old_beta_l=updated_beta_l)
                    updated_beta_list[idx].append(updated_beta_l)
                    beta[idx] = updated_beta_l  # update beta_l after sampling
                    updated_tau_l = self.update_tau(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                    tau=tau,
                                                    old_tau_l=updated_tau_l)
                    updated_tau_list[idx].append(updated_tau_l)
                    tau[idx] = updated_tau_l
        keep_num = -5000
        # update particle parameters
        self.lambda0 = np.average(updated_lambda0_array[keep_num:])
        self.beta = np.average(np.array(updated_beta_list)[:, keep_num:], axis=1)
        self.tau = np.average(np.array(updated_tau_list)[:, keep_num:], axis=1)

    # noinspection PyUnboundLocalVariable,SpellCheckingInspection
    # def update_all_hyperparameters(self, shape_lambda0, scale_lambda0, shape_beta, scale_beta,
    #                                shape_tau, scale_tau, n, N: int = 10000):
    #     """
    #     update lambda0, beta, tau
    #     :param N: sample numbers
    #     :param shape_lambda0: gamma prior parameter for lambda0
    #     :param scale_lambda0: gamma prior parameter for lambda0
    #     :param shape_beta: gamma prior parameter for beta
    #     :param scale_beta: gamma prior parameter for beta
    #     :param shape_tau: gamma prior parameter for tau
    #     :param scale_tau: gamma prior parameter for tau
    #     :param n: event number
    #     :return:
    #     """
    #     # draw candidate lambda0
    #     lambda0_candi_arr = np.random.gamma(shape_lambda0, scale_lambda0, N)
    #     # vectorize func
    #     lambda0_prior = np.vectorize(partial(self.gamma_dist, shape_lambda0, scale_lambda0))
    #     # calculate prior for candidate lambda0
    #     lambda0_p_prior_arr = lambda0_prior(lambda0_candi_arr)
    #
    #     # draw candidate beta
    #     beta_candi_mat = np.random.gamma(shape_beta, scale_beta, (N, self.L))
    #     # vectorize func
    #     beta_prior = np.vectorize(partial(self.gamma_dist, shape_beta, scale_beta))
    #     # calculate prior for candidate beta
    #     beta_p_prior_mat = beta_prior(beta_candi_mat)
    #
    #     # draw candidate tau
    #     tau_candi_mat = np.random.gamma(shape_tau, scale_tau, (N, self.L))
    #     # vectorize func
    #     tau_prior = np.vectorize(partial(self.gamma_dist, shape_tau, scale_tau))
    #     # calculate prior for candidate tau
    #     tau_p_prior_mat = tau_prior(tau_candi_mat)
    #
    #     # calculate log-likelihood
    #     log_hawkes_likelihood_func = np.vectorize(partial(self.log_hawkes_likelihood, n), signature='(),(n),(n)->()')
    #     log_hawkes_likelihood_arr = log_hawkes_likelihood_func(lambda0_candi_arr, beta_candi_mat, tau_candi_mat)
    #
    #     # normalize log-likelihood
    #     log_hawkes_likelihood_arr = np.exp(log_hawkes_likelihood_arr - np.max(log_hawkes_likelihood_arr))
    #     log_hawkes_likelihood_arr = log_hawkes_likelihood_arr / np.sum(log_hawkes_likelihood_arr)
    #
    #     # calculate sample weight
    #     weight_arr = log_hawkes_likelihood_arr * lambda0_p_prior_arr * np.prod(beta_p_prior_mat, axis=1) * \
    #                  np.prod(tau_p_prior_mat, axis=1)
    #     # normalize weight
    #     weight_arr = weight_arr / np.sum(weight_arr)
    #     print(f'weight_arr: {weight_arr}')
    #
    #     # new hyperparameters
    #     self.lambda0 = weight_arr @ lambda0_candi_arr
    #     self.beta = weight_arr @ beta_candi_mat
    #     self.tau = weight_arr @ tau_candi_mat

    # noinspection SpellCheckingInspection
    def update_log_particle_weight(self, old_particle_weight, n: int):
        """
        calculate and update log particle weight
        :param old_particle_weight: former event's particle weight
        :param n: The current sample number (sample order), should be greater than or equal to 1
        :return:
        """
        # calculate lambda_prime
        if n == 1:
            lambda_prime = self.lambda0 * self.timestamp_array[n - 1]
        else:
            lambda_prime = self.lambda0 * (self.timestamp_array[n - 1] - self.timestamp_array[n - 2])
            # delta_tn_array = self.timestamp_array[n - 1] - self.timestamp_array[: n - 1]
            # delta_tn_minus_1_array = self.timestamp_array[n - 2] - self.timestamp_array[: n - 1]
            #
            # # calculate kappa history
            # base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
            # base_kernel_mat = base_kernel_for_delta_t_vec(delta_tn_array, self.beta, self.tau).T
            # kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c[: n - 1]
            # kappa_history_count = np.count_nonzero(kappa_history, axis=1).reshape(-1, 1)
            # kappa_n_minus_1_nonzero_index = np.argwhere(kappa_history[-1] != 0)[:, 0]
            #
            # # calculate sharing coefficient
            # exp_term_coefficient_nominator = np.einsum('lk,l->lk', self.w, self.beta * self.tau)
            #
            # # calculate first exp term: tn-ti
            # delta_t_divide_tau = np.vectorize(np.divide, signature='(),(l)->(l)')
            # exp_term_tn = np.exp(delta_t_divide_tau(delta_tn_array, self.tau))
            # exp_times_coef_term_tn = np.einsum('lk,tl->tk', exp_term_coefficient_nominator, exp_term_tn) * self.c[
            #                                                                                                : n - 1]
            # exp_times_coef_term_tn = exp_times_coef_term_tn / kappa_history_count
            # first_term = np.sum(exp_times_coef_term_tn[:, kappa_n_minus_1_nonzero_index])
            #
            # # calculate second exp term: t_n-1 - ti
            # exp_term_tn_1 = np.exp((delta_t_divide_tau(delta_tn_minus_1_array, self.tau)))
            # exp_times_coef_term_tn_minus_1 = np.einsum('lk,tl->tk', exp_term_coefficient_nominator,
            #                                            exp_term_tn_1) * self.c[
            #                                                             : n - 1]
            # exp_times_coef_term_tn_minus_1 = exp_times_coef_term_tn_minus_1 / kappa_history_count
            # second_term = np.sum(exp_times_coef_term_tn_minus_1[:, kappa_n_minus_1_nonzero_index])
            #
            # # calculate lambda_prime
            # lambda_prime = self.lambda0 * (self.timestamp_array[n - 1] - self.timestamp_array[n - 2]) - \
            #                (first_term - second_term)
        # calculate log likelihood for timestamp
        log_likelihood_timestamp = np.log(lambda_prime) - lambda_prime

        # log likelihood for text
        kappa_n_minus_1_nonzero_index = np.argwhere(self.kappa_n != 0)[:, 0]
        vn_avg = np.einsum('ij->i', self.v[:, kappa_n_minus_1_nonzero_index]) / np.count_nonzero(self.kappa_n)
        Tn = transfer_multi_dist_result_to_vec(self.T_array)[n - 1]  # shape=(S, )
        count_dict = Counter(Tn)
        log_likelihood_text = 0
        for k, v in count_dict.items():
            log_likelihood_text += v * np.log(vn_avg[k])
        # Calculate the updated log particle weight
        self.log_particle_weight = np.log(old_particle_weight) + log_likelihood_timestamp + log_likelihood_text


# noinspection PyPep8Naming,SpellCheckingInspection,PyShadowingNames
class Particle_Filter:
    """
    This class controls weight updating, normalization, and resampling of all particles (in parallel)
    """

    def __init__(self, timestamp_array: np.ndarray, T_array: np.ndarray,
                 n_particle: int, word_dict: np.ndarray, L: int = 3):
        """
        Generates particles and initializes particle weights

        :param timestamp_array: timestamp vector
        :param T_array: text
        :param n_particle: number of particles
        :param word_dict: dictionary
        :param L: The number of base kernels
        """
        assert len(timestamp_array) == T_array.shape[0]
        self.n_sample = T_array.shape[0]
        self.n_particle = n_particle
        self.word_corpus = word_dict
        self.particle_list = [Particle(word_dict=word_dict, timestamp_array=timestamp_array, T_array=T_array, L=L) for
                              i in np.arange(self.n_particle)]
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)

    def get_particle_list(self):
        return self.particle_list

    def get_particle_num(self):
        return self.n_particle

    def get_partcie_weight_arr(self):
        return self.particle_weight_arr

    def normalize_particle_weight(self):
        """
        Normalize particle weights (map weights to 0-1 interval and sum to 1)
        :return:
        """
        self.particle_weight_arr = np.exp(self.particle_weight_arr - np.max(self.particle_weight_arr))
        self.particle_weight_arr = self.particle_weight_arr / np.sum(self.particle_weight_arr)
        logging.info(f'particle weight: {self.particle_weight_arr}')

    def resample_particles(self):
        """
        Resampling particles, resampling n_particle particles
        :return:
        """
        new_particle_list = []
        sorted_particle_index = np.argsort(self.particle_weight_arr)  # 得到的是粒子权重升序排列的索引
        sorted_particle_weight = self.particle_weight_arr[sorted_particle_index]  # 升序排列后的粒子权重
        # 构造粒子对应的权重区间
        for i in np.arange(self.n_particle - 1, -1, -1):
            sorted_particle_weight[i] = np.sum(sorted_particle_weight[: i + 1])
        # 重新采样n_particle个粒子
        for i in np.arange(self.n_particle):
            u = np.random.rand()
            nearest_elem_idx = np.argmin(np.abs(u - sorted_particle_weight))
            if u <= sorted_particle_weight[nearest_elem_idx]:
                new_particle = self.particle_list[nearest_elem_idx]
            else:
                new_particle = self.particle_list[nearest_elem_idx + 1]
            new_particle_list.append(new_particle)
        # 更新粒子列表，重新设置粒子权重
        self.particle_list = new_particle_list
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)

    def generate_first_event_status_for_each_particle(self, particle_idx_pair: Tuple[int, Particle]):
        """
        Generate the state corresponding to the first event for each particle
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        logging.info(f'[event 1, paricle {particle_idx + 1}] Sampling particle status')
        particle.sample_first_particle_event_status()
        # Update hyperparameters and triggering kernels
        logging.info(f'[event 1, paricle {particle_idx + 1}] Updating hyperparameters')
        particle.mh_update(n=1)
        # Calculate and update the log particle weights
        logging.info(f'[event 1, paricle {particle_idx + 1}] Updating particle weight')
        particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=1)
        return particle_idx, particle

    def generate_following_event_status_for_each_paritcle(self, n: int, particle_idx_pair: Tuple[int, Particle]):
        """

        :param n:
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        logging.info(f'[event {n}, particle {particle_idx + 1}] Sampling particle status')
        particle.sample_particle_following_event_status(n)
        logging.info(f'[event {n}, particle {particle_idx + 1}] Updating hyperparameters')
        particle.mh_update(n=n)
        logging.info(f'[event {n}, particle {particle_idx + 1}] Updating particle weight')
        particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=n)
        return particle_idx, particle

    def update_particle_weight_arr(self, particle_index_list: List[Tuple[int, Particle]]):
        """
        Update particle weight list
        :param particle_index_list:
        :return:
        """
        for idx, particle in particle_index_list:
            self.particle_weight_arr[idx] = particle.log_particle_weight


# noinspection SpellCheckingInspection
if __name__ == '__main__':
    # Generate test data
    # parameters
    # n_sample = int(sys.argv[1])
    # n_particle = int(sys.argv[2])

    n_sample = 10
    n_particle = 50

    logging.info(f'set n_sample to {n_sample}')
    logging.info(f'set n_particle to {n_particle}')

    # noinspection SpellCheckingInspection
    ibhp = IBHP(n_sample)
    ibhp.generate_data()
    logging.info(f'\n{"-" * 40} Observational data generated {"-" * 40}\n')
    logging.info(f'Timestamp: {ibhp.timestamp_array}\n')
    logging.info(f'Text: {transfer_multi_dist_result_to_vec(ibhp.text)}\n')
    word_dict = np.arange(1000)
    logging.info(f'Dictionary: {word_dict}\n')

    # filtering
    # noinspection PyBroadException,SpellCheckingInspection
    logging.info(f'\n{"-" * 40}  Start particle filter parameter estimation {"-" * 40}\n')
    pf = Particle_Filter(timestamp_array=ibhp.timestamp_array, T_array=ibhp.text, n_particle=n_particle,
                         word_dict=word_dict, L=3)
    particle_index_pair_list = [(idx, particle) for idx, particle in enumerate(pf.get_particle_list())]
    # event 1 status
    pool_event_1 = Pool(cpu_count())
    particle_index_pair_list = list(pool_event_1.map(pf.generate_first_event_status_for_each_particle,
                                                     particle_index_pair_list))
    pool_event_1.close()
    pool_event_1.join()
    pf.update_particle_weight_arr(particle_index_list=particle_index_pair_list)
    pf.normalize_particle_weight()
    logging.info(f'[event 1] Normalized particle weight: \n{pf.get_partcie_weight_arr()}')

    # output
    # particle weight
    # noinspection DuplicatedCode
    p_weight_arr = pf.get_partcie_weight_arr()
    logging.info(f'weight of all particles: {p_weight_arr}\n')

    # Hyperparameters weighted average
    lam_0_arr = np.array([particle.lambda0 for idx, particle in particle_index_pair_list])
    lam_0 = np.average(lam_0_arr, weights=p_weight_arr)
    beta_arr = np.array([particle.beta.reshape(-1) for idx, particle in particle_index_pair_list])
    beta = np.average(beta_arr, axis=0, weights=p_weight_arr)
    tau_arr = np.array([particle.tau.reshape(-1) for idx, particle in particle_index_pair_list])
    tau = np.average(tau_arr, axis=0, weights=p_weight_arr)
    logging.info(f'weighted average of three parameters: \n lambda_0: {lam_0}\n beta: {beta}\n tau: {tau}\n')

    N_eff = 1 / np.sum(np.square(pf.get_partcie_weight_arr()))
    if N_eff < 2 / 3 * pf.get_particle_num():
        logging.info(f'[event 1] Resampling particles')
        pf.resample_particles()

    # event 2~n status
    # noinspection SpellCheckingInspection
    for n in np.arange(2, n_sample + 1):
        # process pool for event n
        pool_event_n = Pool(cpu_count())
        particle_index_pair_list = list(
            pool_event_n.map(partial(pf.generate_following_event_status_for_each_paritcle, n),
                             particle_index_pair_list))
        pool_event_n.close()
        pool_event_n.join()
        pf.update_particle_weight_arr(particle_index_list=particle_index_pair_list)
        pf.normalize_particle_weight()

        # output
        logging.info(f'[event {n}] Normalized particle weight: {pf.get_partcie_weight_arr()}')
        # particle weight
        # noinspection DuplicatedCode
        p_weight_arr = pf.get_partcie_weight_arr()
        logging.info(f'weight of all particles: {p_weight_arr}\n')

        # Hyperparameters weighted average
        lam_0_arr = np.array([particle.lambda0 for idx, particle in particle_index_pair_list])
        lam_0 = np.average(lam_0_arr, weights=p_weight_arr)
        beta_arr = np.array([particle.beta.reshape(-1) for idx, particle in particle_index_pair_list])
        beta = np.average(beta_arr, axis=0, weights=p_weight_arr)
        tau_arr = np.array([particle.tau.reshape(-1) for idx, particle in particle_index_pair_list])
        tau = np.average(tau_arr, axis=0, weights=p_weight_arr)
        logging.info(f'weighted average of three parameters: \n lambda_0: {lam_0}\n beta: {beta}\n tau: {tau}\n')

        if n == n_sample:
            break
        N_eff = 1 / np.sum(np.square(pf.get_partcie_weight_arr()))
        if N_eff < 2 / 3 * pf.get_particle_num():
            logging.info(f'[event {n}] Resampling particles')
            pf.resample_particles()
