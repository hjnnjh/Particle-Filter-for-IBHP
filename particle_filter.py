#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# noinspection SpellCheckingInspection
"""
@File    :   particle_filter.py
@Time    :   2021/11/10 22:01
@Author  :   Jinnan Huang
@Contact :   jinnan_huang@stu.xjtu.edu.cn
@Desc    :   None
"""
import logging
import os.path
from collections import Counter
from copy import deepcopy
from datetime import datetime
from functools import partial
from typing import List, Tuple

import numpy as np
import pyro
import pyro.distributions as dist
import scipy.special as spe
import scipy.stats as sta
import torch
from sklearn.utils.extmath import cartesian
from tqdm import tqdm

from IBHP_simulation import IBHP


# ------------------------------ utils ------------------------------

# noinspection PyPep8Naming,PyShadowingNames
def transfer_multi_dist_result_to_vec(T_array: np.ndarray):
    """
    transform the data matrix generated by multinomial distribution to a concrete word matrix
    :param T_array: text vector
    :return:
    """

    # noinspection DuplicatedCode
    def transfer_occurrence_data(idx: list, data: list):
        res = []
        res.extend([[idx[i]] * data[i] for i in range(len(idx))])
        res = [i for k in res for i in k]
        return res

    word_index = np.argwhere(T_array != 0)
    index_list = [word_index[word_index[:, 0] == n][:, 1].tolist() for n in np.unique(word_index[:, 0])]
    word_occurrence_list = [T_array[i, idx].tolist() for i, idx in enumerate(index_list)]
    word_corpus_mat = np.array([transfer_occurrence_data(index_list[i], word_occurrence_list[i])
                                for i in range(len(index_list))])
    return word_corpus_mat


# ------------------------------ main classes ------------------------------

# noinspection PyMissingConstructor,DuplicatedCode,PyPep8Naming,PyShadowingNames,SpellCheckingInspection
class Particle(IBHP):
    # noinspection SpellCheckingInspection
    """
    This class implements all the steps of single particle sampling, hyperparameter updating and particle weight
    calculation.
    """

    # noinspection SpellCheckingInspection
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S",
                        level=logging.INFO)

    # noinspection PyShadowingNames
    def __init__(self, word_dict: np.ndarray,
                 timestamp_array: np.ndarray, T_array: np.ndarray,
                 simulation_w: np.ndarray = None, simulation_v: np.ndarray = None, fix_params: bool = False,
                 L: int = 3, random_seed: int = None):
        """
        :param T_array: text vector
        :param L: Number of base kernels
        :param word_dict: dictionary
        """
        super(Particle, self).__init__()
        self.random_seed = random_seed
        self.fix_params = fix_params
        self.log_particle_weight = None
        self.T_array = T_array
        self.timestamp_array = timestamp_array
        self.L = L
        self.word_dict = word_dict
        self.S = len(self.word_dict)
        self.lambda_k_array_mat = None
        self.lambda_k_array = None
        if fix_params:
            self.simulation_v = simulation_v
            self.simulation_w = simulation_w
            assert isinstance(self.simulation_w, np.ndarray)
            assert isinstance(self.simulation_v, np.ndarray)
            assert self.simulation_w.shape[1] == self.simulation_v.shape[1]
            self.real_factor_num = self.simulation_w.shape[1]
        if self.random_seed:
            np.random.seed(self.random_seed)
            torch.manual_seed(self.random_seed)

    def calculate_lambda_k(self, n):
        """
        calculate lambda_k array for each event
        :param n:
        :return:
        """
        if n == 1:
            self.lambda_k_array = self.w.T @ self.beta
        elif n >= 2:
            delta_t_array = self.timestamp_array[n - 1] - self.timestamp_array[: n]

            base_kernel_for_delta_t_vec = np.vectorize(self.base_kernel_l, signature='(n),(),()->(n)')
            base_kernel_mat = base_kernel_for_delta_t_vec(delta_t_array, self.beta, self.tau).T  # t_i for each row
            kappa_history = np.einsum('lk,tl->tk', self.w, base_kernel_mat) * self.c
            kappa_history_count = np.count_nonzero(self.c, axis=1).reshape(-1, 1)
            self.lambda_k_array = np.sum(kappa_history / kappa_history_count, axis=0)

    def collect_factor_intensity(self, n):
        """
        Collect intensity of all factors generated during the simulation
        :param n:
        :return:
        """
        if n == 1:
            self.lambda_k_array_mat = self.lambda_k_array.reshape(1, -1)
        elif n >= 2:
            zero_num = self.lambda_k_array.shape[0] - self.lambda_k_array_mat[-1].shape[0]
            if zero_num:
                self.lambda_k_array_mat = np.hstack((self.lambda_k_array_mat,
                                                     np.zeros((self.lambda_k_array_mat.shape[0], zero_num))))
            self.lambda_k_array_mat = np.vstack((self.lambda_k_array_mat, self.lambda_k_array))

    def sample_first_particle_event_status(self, parameter_fixed: bool = False):
        """
        Generate the particle state corresponding to the first Event
        :return:
        """
        # Initialize lambda0, beta, tau
        if parameter_fixed:
            pass
        else:
            self.lambda0 = 1.5
            self.beta = np.array([1, 1, 1])  # array, shape=(L, )
            self.tau = np.array([0.5, 0.4, 0.3])  # array, shape=(L, )

        self.K = 0
        self.w_0 = np.array([1 / self.L] * self.L)  # array, shape=(L, )
        self.v_0 = np.array([1 / self.S] * self.S)  # array, shape(S, )

        # Generate K
        self.K = np.random.poisson(self.lambda0)  # K: int
        while self.K == 0:
            logging.info(f'[event 1] initial topic number K is 0, regenerate')
            self.K = np.random.poisson(self.lambda0)
        # Generate the topic occurrence matrix c, set all c_k=1
        self.c = np.ones((1, self.K))  # matrix, shape=(1, self.K)
        if self.fix_params:  # set w equals w generated in simulation process
            if self.real_factor_num >= self.K:
                self.w = self.simulation_w[:, : self.K]
            else:
                with pyro.plate('wk_1', self.K - self.real_factor_num):
                    w = pyro.sample('w_1', dist.Dirichlet(torch.from_numpy(self.w_0)))
                w = w.numpy().T  # matrix, shape=(L, K), each column is the weight of each k
                self.w = np.hstack((self.simulation_w[:, : self.K], w))
        else:
            # Generate w
            with pyro.plate('wk_1', self.K):
                self.w = pyro.sample('w_1', dist.Dirichlet(torch.from_numpy(self.w_0)))
            self.w = self.w.numpy().T  # matrix, shape=(L, K), each column is the weight distribution for each k
        if self.fix_params:  # set v equals v generated in simulation process
            if self.real_factor_num >= self.K:
                self.v = self.simulation_v[:, : self.K]
            else:
                with pyro.plate('vk_1', self.K - self.real_factor_num):
                    v = pyro.sample('v_1', dist.Dirichlet(torch.from_numpy(self.v_0)))
                v = v.numpy().T  # matrix, shape=(L, K), each column is the weight of each k
                self.v = np.hstack((self.simulation_w[:, : self.K], v))
        else:
            # Generate v
            with pyro.plate('vk_1', self.K):
                self.v = pyro.sample('v_1', dist.Dirichlet(torch.from_numpy(self.v_0)))
            self.v = self.v.numpy().T  # matrix, shape=(S, K), Each column is a distribution of words for each k
        # calculate lambda_1
        self.calculate_lambda_k(1)
        c_n = np.argwhere(self.c[-1] != 0)[:, 0]
        self.lambda_tn_array = np.array([np.sum(self.lambda_k_array[c_n])])
        self.collect_factor_intensity(1)
        print(f'event 1 history lambda_k shape: {self.lambda_k_array_mat.shape}')
        print(f'event 1 c shape: {self.c.shape}')

    def sample_particle_following_event_status(self, n: int):
        """
        Generate the particle state corresponding to the following event
        :param n: n-th event
        :return:
        """
        # Calculate the probability of generating c from the existing K topics
        p = self.lambda_k_array / ((self.lambda0 / self.K) + self.lambda_k_array)
        # Generate topic occurrence vectors with K topics
        vfunc_generate_old_c = np.vectorize(self.generate_c)
        c_old = vfunc_generate_old_c(p)
        # Generate K+
        k_plus = np.random.poisson(self.lambda0 / (self.lambda0 + np.sum(self.lambda_k_array)), 1)[0]
        # When K+ is equal to 0, check whether c_old is all 0, and if it is all 0, regenerate c_old
        if k_plus == 0:
            while np.all(c_old == 0):
                logging.info(f'[event {n}]When K+ is 0, c_old is also all 0, and c_old is regenerated')
                c_old = vfunc_generate_old_c(p)
        # update K
        self.K = self.K + k_plus
        if k_plus:
            # If K+ is greater than 0, initialize a new topic occurrence vector
            c_new = np.ones(k_plus)
            c = np.hstack((c_old, c_new))
            self.c = np.hstack((self.c, np.zeros((self.c.shape[0], k_plus))))  # Complete the existing c matrix with 0
            self.c = np.vstack((self.c, c))  # Add the new c vector to the c matrix
            # fix w matrix
            if self.fix_params:
                if self.real_factor_num >= self.K:
                    self.w = self.simulation_w[:, :self.K]
                else:
                    with pyro.plate(f'wk_{n}', self.K - self.real_factor_num):
                        w = pyro.sample(f'new_w_{n}', dist.Dirichlet(torch.from_numpy(self.w_0)))
                    w = w.numpy().T
                    self.w = np.hstack((self.simulation_w[:, :self.K], w))
                w_new = self.w[:, -k_plus:]
            else:
                # If K+ is greater than 0, generate a new w_k, update self.w
                with pyro.plate(f'wk_{n}', k_plus):
                    w_new = pyro.sample(f'new_w_{n}', dist.Dirichlet(torch.from_numpy(self.w_0)))
                w_new = w_new.numpy().T
                self.w = np.hstack((self.w, w_new))
            # fix v matrix
            if self.fix_params:
                if self.real_factor_num >= self.K:
                    self.v = self.simulation_v[:, :self.K]
                else:
                    with pyro.plate(f'vk_{n}', self.K - self.real_factor_num):
                        v = pyro.sample(f'new_v_{n}', dist.Dirichlet(torch.from_numpy(self.v_0)))
                    v = v.numpy().T
                    self.v = np.hstack((self.simulation_v[:, :self.K], v))
            else:
                # If K+ is greater than 0, generate a new v_k, update self.v
                with pyro.plate(f'vk_{n}', k_plus):
                    v_new = pyro.sample(f'new_v_{n}', dist.Dirichlet(torch.from_numpy(self.v_0)))
                v_new = v_new.numpy().T
                self.v = np.hstack((self.v, v_new))
        else:
            self.c = np.vstack((self.c, c_old))
        # calculate lambda_tn
        self.calculate_lambda_k(n)
        c_n = np.argwhere(self.c[-1] != 0)[:, 0]
        self.lambda_tn_array = np.append(self.lambda_tn_array,
                                         np.sum(self.lambda_k_array[c_n]))
        self.collect_factor_intensity(n)
        print(f'event {n} history lambda_k shape: {self.lambda_k_array_mat.shape}')
        print(f'event {n} c shape: {self.c.shape}')

    # noinspection SpellCheckingInspection
    def log_hawkes_likelihood(self, n, lambda0, beta, tau):
        """
        log hawkes likelihood
        :param tau: candidate tau
        :param beta: candidate beta
        :param lambda0: candidate lambda0
        :param n: The current sample order, which must be greater than or equal to 1
        :return:
        """
        # ---------------------- intergal term ----------------------
        divide_ufunc = np.vectorize(np.divide, signature='(),(l)->(l)')
        if n == 1:
            log_integral_term = - lambda0 * self.timestamp_array[0]
        else:
            sum_term = 0
            for i in np.arange(2, n + 1):
                integral_delta_ti_1_tj = self.timestamp_array[i - 2] - self.timestamp_array[: i - 1]
                exp_ti_1_tj = np.exp(- divide_ufunc(integral_delta_ti_1_tj, tau))
                integral_delta_ti_tj = self.timestamp_array[i - 1] - self.timestamp_array[: i - 1]
                exp_ti_tj = np.exp(- divide_ufunc(integral_delta_ti_tj, tau))
                exp_term = exp_ti_tj - exp_ti_1_tj  # (t, l)
                exp_term = beta * tau * exp_term
                exp_term = np.einsum('lk,tl->tk', self.w, exp_term) * self.c[: i - 1]
                kappa_j_count = np.count_nonzero(self.c[: i - 1], axis=1).reshape(-1, 1)
                c_i = np.argwhere(self.c[i - 1] != 0)[:, 0]  # nonzero element index for current event
                sum_j_integral = np.sum((exp_term / kappa_j_count)[:, c_i])
                sum_term += sum_j_integral
            print(f'event {n} sum_term: {sum_term}')
            log_integral_term = - lambda0 * self.timestamp_array[0] + sum_term

        # ---------------------- product term ----------------------
        log_prod_term = 0
        for i in np.arange(1, n + 1):
            prod_delta_ti_tj = self.timestamp_array[i - 1] - self.timestamp_array[: i]
            prod_exp_term = np.exp(- divide_ufunc(prod_delta_ti_tj, tau))
            prod_exp_term = beta * prod_exp_term
            prod_exp_term = np.einsum('lk,tl->tk', self.w, prod_exp_term) * self.c[: i]
            kappa_j_count_prod = np.count_nonzero(self.c[: i], axis=1).reshape(-1, 1)
            c_i = np.argwhere(self.c[i - 1] != 0)[:, 0]  # nonzero element index for current event
            sum_j_prod = np.sum((prod_exp_term / kappa_j_count_prod)[:, c_i])
            log_sum_j_prod = np.log(sum_j_prod)
            log_prod_term += log_sum_j_prod
        print(f'event {n} log_prod_term: {log_prod_term}')

        # ---------------------- log hawkes likelihood ----------------------
        log_hawkes_likelihood = log_integral_term + log_prod_term
        return log_hawkes_likelihood

    # ----------------------------------- Metropolis Hastings Algorithm -----------------------------------

    def update_lambda0(self, n: int, n_mh: int, beta, tau, lambda0, parameter_a=3):
        """
        update lambda0 using Metropolis-Hastings Algorithm
        :param parameter_a:
        :param lambda0:
        :param tau: updated tau
        :param beta: update beta
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :return:
        """
        lambda0_old = lambda0
        lambda0_candidate = sta.gamma.rvs(
            a=parameter_a, scale=lambda0_old)  # draw candidate lambda0 from a parameter-specified distribution
        lambda0_old_prior = sta.gamma.pdf(x=lambda0_old, a=parameter_a,
                                          scale=parameter_a / 2)  # probability of lambda0_old prior distribution
        lambda0_candidate_prior = sta.gamma.pdf(x=lambda0_candidate,
                                                a=parameter_a, scale=parameter_a / 2)  # probability of lambda0_candidate prior distribution

        # likelihood
        log_hawkes_likelihood_lambda0_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0_old,
                                                                       beta=beta, tau=tau)
        log_hawkes_likelihood_lambda0_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0_candidate,
                                                                             beta=beta, tau=tau)

        # probability of proposal distribution
        lambda0_candidate_proposal = sta.gamma.pdf(x=lambda0_old, a=parameter_a, scale=lambda0_candidate)
        lambda0_old_proposal = sta.gamma.pdf(x=lambda0_candidate, a=parameter_a, scale=lambda0_old)

        log_accept_ratio = np.log(lambda0_candidate_prior) + log_hawkes_likelihood_lambda0_candidate + np.log(
            lambda0_candidate_proposal) - np.log(lambda0_old_prior) - log_hawkes_likelihood_lambda0_old - np.log(
            lambda0_old_proposal)
        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return lambda0_candidate
        else:
            return lambda0_old

    def update_beta(self, n, n_mh, index, lambda0, beta, tau, parameter_a=2.5):
        """
        update each beta using Metropolis-Hastings Algorithm
        :param parameter_a:
        :param beta:
        :param tau: updated tau
        :param lambda0: updated lambda0
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :param index: the index of beta in beta array
        :return:
        """

        def copy_beta(replace_index, replace_value, input_beta):
            """
            copy self.beta, replace specific value
            :param input_beta:
            :param replace_index:
            :param replace_value:
            :return:
            """
            output_beta = deepcopy(input_beta)
            output_beta[replace_index] = replace_value
            return output_beta

        beta_l_old = beta[index]

        # candidate rvs
        beta_l_candidate = sta.gamma.rvs(a=parameter_a, scale=beta_l_old)

        # prior
        beta_l_old_prior = sta.gamma.pdf(x=beta_l_old, a=parameter_a, scale=parameter_a / 2)
        beta_l_candidate_prior = sta.gamma.pdf(x=beta_l_candidate, a=parameter_a, scale=parameter_a / 2)

        # likelihood
        beta_old = copy_beta(index, beta_l_old, beta)
        beta_candidate = copy_beta(index, beta_l_candidate, beta)
        log_hawkes_likelihood_beta_l_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0, beta=beta_old,
                                                                      tau=tau)
        log_hawkes_likelihood_beta_l_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0,
                                                                            beta=beta_candidate, tau=tau)

        # proposal
        beta_l_candidate_proposal = sta.gamma.pdf(x=beta_l_old, a=parameter_a, scale=beta_l_candidate)
        beta_l_old_proposal = sta.gamma.pdf(x=beta_l_candidate, a=parameter_a, scale=beta_l_old)

        log_accept_ratio = np.log(beta_l_candidate_prior) + log_hawkes_likelihood_beta_l_candidate + np.log(
            beta_l_candidate_proposal) - np.log(beta_l_old_prior) - log_hawkes_likelihood_beta_l_old - np.log(
            beta_l_old_proposal)

        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return beta_l_candidate
        else:
            return beta_l_old

    def update_tau(self, n, n_mh, index, lambda0, beta, tau, parameter_a=1):
        """
        update each tau using Metropolis-Hastings Algorithm
        :param parameter_a:
        :param tau:
        :param beta:
        :param lambda0:
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_mh: The current nth sample of mh algorithm
        :param index: the index of tau in tau array
        :return:
        """

        def copy_tau(replace_index, replace_value, input_tau):
            """
            copy self.tau, replace specific value
            :param input_tau:
            :param replace_index:
            :param replace_value:
            :return:
            """
            output_tau = deepcopy(input_tau)
            output_tau[replace_index] = replace_value
            return output_tau

        tau_l_old = tau[index]

        # candidate rvs
        tau_l_candidate = sta.gamma.rvs(a=parameter_a, scale=tau_l_old)

        # prior
        tau_l_old_prior = sta.gamma.pdf(x=tau_l_old, a=parameter_a, scale=parameter_a / 2)
        tau_l_candidate_prior = sta.gamma.pdf(x=tau_l_candidate, a=parameter_a, scale=parameter_a / 2)

        # likelihood
        tau_old = copy_tau(index, tau_l_old, tau)
        tau_candidate = copy_tau(index, tau_l_candidate, tau)
        log_hawkes_likelihood_tau_l_old = self.log_hawkes_likelihood(n=n, lambda0=lambda0, beta=beta,
                                                                     tau=tau_old)
        log_hawkes_likelihood_tau_l_candidate = self.log_hawkes_likelihood(n=n, lambda0=lambda0,
                                                                           beta=beta, tau=tau_candidate)

        # proposal
        tau_l_candidate_proposal = sta.gamma.pdf(x=tau_l_old, a=parameter_a, scale=tau_l_candidate)
        tau_l_old_proposal = sta.gamma.pdf(x=tau_l_candidate, a=parameter_a, scale=tau_l_old)

        log_accept_ratio = np.log(tau_l_candidate_prior) + log_hawkes_likelihood_tau_l_candidate + np.log(
            tau_l_candidate_proposal) - np.log(tau_l_old_prior) - log_hawkes_likelihood_tau_l_old - np.log(
            tau_l_old_proposal)

        u = sta.uniform.rvs(0, 1)
        if u == 0 or np.log(u) <= log_accept_ratio:
            return tau_l_candidate
        else:
            return tau_l_old

    def mh_update(self, n, n_iter=10000):
        """
        update parameters
        :param n: The current sample order, which must be greater than or equal to 1
        :param n_iter: number of iterations
        :return:
        """
        beta = deepcopy(self.beta)
        tau = deepcopy(self.tau)
        updated_lambda0, updated_beta_l, updated_tau_l = None, None, None
        updated_lambda0_array = None
        updated_beta_list = [[] for i in np.arange(self.L)]
        updated_tau_list = [[] for i in np.arange(self.L)]

        mh_iter = tqdm(np.arange(n_iter))

        for i in mh_iter:
            mh_iter.set_description(f'[event {n}] sampling parameters through MH')
            if i == 0:
                updated_lambda0 = self.update_lambda0(n=n, n_mh=i + 1, lambda0=self.lambda0, beta=beta, tau=tau)
                updated_lambda0_array = np.array([updated_lambda0])
                for idx in np.arange(self.L):
                    updated_beta_l = self.update_beta(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                      tau=tau)
                    updated_beta_list[idx].append(updated_beta_l)
                    beta[idx] = updated_beta_l  # update beta_l after sampling
                    updated_tau_l = self.update_tau(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                    tau=tau)
                    updated_tau_list[idx].append(updated_tau_l)
                    tau[idx] = updated_tau_l
            else:
                updated_lambda0 = self.update_lambda0(n=n, n_mh=i + 1, beta=beta, tau=tau,
                                                      lambda0=updated_lambda0)
                updated_lambda0_array = np.append(updated_lambda0_array, updated_lambda0)
                for idx in np.arange(self.L):
                    updated_beta_l = self.update_beta(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                      tau=tau)
                    updated_beta_list[idx].append(updated_beta_l)
                    beta[idx] = updated_beta_l  # update beta_l after sampling
                    updated_tau_l = self.update_tau(n=n, n_mh=i + 1, index=idx, lambda0=updated_lambda0, beta=beta,
                                                    tau=tau)
                    updated_tau_list[idx].append(updated_tau_l)
                    tau[idx] = updated_tau_l

        burning = int(n_iter - n_iter / 10)
        self.lambda0 = np.average(updated_lambda0_array[burning:])
        self.beta = np.average(np.array(updated_beta_list).T[burning:], axis=0)
        self.tau = np.average(np.array(updated_tau_list).T[burning:], axis=0)

        print(f'[event {n}] updated lambda0: {self.lambda0}')
        print(f'[event {n}] updated beta: {self.beta}')
        print(f'[event {n}] updated tau: {self.tau}')

    # ----------------------------------- Randomly sampling from prior -----------------------------------

    # noinspection PyUnboundLocalVariable,SpellCheckingInspection
    def update_hyperparameter(self, n, random_N: int = 1000, cartesian_N: int = 25, method: str = 'average'):
        """
        update lambda0, beta, tau
        :param random_N: sample numbers for random
        :param method: 'maximum' or 'average', if 'maximum' received, choose the sample that makes the
        likelihood greatest.
        :param cartesian_N: sample numbers for cartesian
        (sample number of lambda0 is the shape of product matrix of beta or tau)
        :param n: event number
        :return:
        """
        # draw candidate beta
        beta_candi_mat_random = sta.gamma.rvs(a=3, size=(random_N, self.L))
        # calculate Cartesian product of beta arrays, the aim is to cover a more comprehensive sample space
        beta_candi_mat_cartesian = cartesian((sta.gamma.rvs(a=3, size=cartesian_N),) * self.L)
        beta_candi_mat = np.vstack((beta_candi_mat_random, beta_candi_mat_cartesian))
        # calculate prior for candidate beta
        beta_p_prior_mat = sta.gamma.pdf(x=beta_candi_mat, a=3)

        # draw candidate tau
        tau_candi_mat_random = sta.gamma.rvs(a=1.5, scale=0.15, size=(random_N, self.L))
        # calculate Cartesian product of tau arrays
        tau_candi_mat_cartesian = cartesian((sta.gamma.rvs(a=1, size=cartesian_N),) * self.L)  # (N, L)
        tau_candi_mat = np.vstack((tau_candi_mat_random, tau_candi_mat_cartesian))
        # calculate prior for candidate tau
        tau_p_prior_mat = sta.gamma.pdf(x=tau_candi_mat, a=1.5, scale=0.15)

        # draw candidate lambda0 from prior
        lambda0_candi_arr = sta.gamma.rvs(a=3, size=random_N + cartesian_N ** self.L)
        # calculate prior for candidate lambda0
        lambda0_p_prior_arr = sta.gamma.pdf(x=lambda0_candi_arr, a=3)

        # calculate log-likelihood
        log_hawkes_likelihood_func = np.vectorize(partial(self.log_hawkes_likelihood, n), signature='(),(n),(n)->()')
        log_hawkes_likelihood_arr = log_hawkes_likelihood_func(lambda0_candi_arr, beta_candi_mat, tau_candi_mat)

        # calculate sample weight
        log_weight_arr = np.log(lambda0_p_prior_arr) + np.sum(np.log(beta_p_prior_mat), axis=1) + np.sum(
            np.log(tau_p_prior_mat), axis=1) + log_hawkes_likelihood_arr

        # normalize weight using softmax func
        weight_arr = spe.softmax(log_weight_arr)

        # new hyperparameters, maximum likelihood sample
        if method is 'maximum':
            best_sample_index = np.argmax(log_hawkes_likelihood_arr)
            self.lambda0 = lambda0_candi_arr[best_sample_index]
            self.beta = beta_candi_mat[best_sample_index]
            self.tau = tau_candi_mat[best_sample_index]
        elif method is 'average':
            # new hyperparameters, weighted average
            self.lambda0 = weight_arr @ lambda0_candi_arr
            self.beta = weight_arr @ beta_candi_mat
            self.tau = weight_arr @ tau_candi_mat
        else:
            raise ValueError('Parameter `method` got an unexpected value')

    # # ----------------------------------- Gradient descent -----------------------------------

    def grad_lambda0(self, n):
        """
        calculate gradient for lambda0
        :param n: event
        :return:
        """
        sum_term_lambda0 = 0
        for i in np.arange(1, n + 1):
            if i == 1:
                sum_term_lambda0 += 1 / self.lambda0
            else:
                delta_ti_tj = self.timestamp_array[i - 1] - self.timestamp_array[: i - 1]
                divide_func = np.vectorize(np.divide, signature='(),(l)->(l)')
                exp_term = np.exp(-divide_func(delta_ti_tj, self.tau))  # (t, l)
                exp_term = self.beta * exp_term
                numerator = np.einsum('lk,tl->tk', self.w, exp_term) * self.c[: i - 1]
                kappa_history_j_count = np.count_nonzero(self.c[: i - 1], axis=1).reshape(-1, 1)
                nominator_sum = np.sum(numerator / kappa_history_j_count)
                each_sum_term = 1 / (self.lambda0 + nominator_sum)
                sum_term_lambda0 += each_sum_term
        grad_lambda0 = self.timestamp_array[-1] - sum_term_lambda0
        return grad_lambda0

    def grad_beta_l(self, n, l_index):
        """
        calculate gradient for each beta
        :param l_index: update the l_index position beta
        :param n: event
        :return:
        """
        tau_l = self.tau[l_index]
        w_kl = self.w[l_index, :]

        # first term
        delta_T = self.timestamp_array[-1] - self.timestamp_array[: n]
        exp_term = np.exp(-delta_T / tau_l) - 1  # (t, )
        numerator = tau_l * exp_term
        multiply = np.vectorize(np.multiply, signature='(k),()->(k)')
        numerator = multiply(w_kl, numerator) * self.c[: n]
        kappa_i_count = np.count_nonzero(self.c[: n], axis=1).reshape(-1, 1)
        first_term = -np.sum(numerator / kappa_i_count)

        # second term
        second_term = 0
        for i in np.arange(1, n + 1):
            if i == 1:
                second_term += 1 / self.lambda0
            else:
                delta_ti_tj = self.timestamp_array[i - 1] - self.timestamp_array[: i - 1]
                exp_term = np.exp(-delta_ti_tj / tau_l)
                sub_numerator = multiply(w_kl, exp_term) * self.c[: i - 1]
                kappa_j_count = np.count_nonzero(self.c[: i - 1], axis=1).reshape(-1, 1)
                numerator = np.sum(sub_numerator / kappa_j_count)

                divide = np.vectorize(np.divide, signature='(),(l)->(l)')
                exp_term = np.exp(-divide(delta_ti_tj, self.tau))
                exp_term = self.beta * exp_term
                sub_numerator = np.einsum('lk,tl->tk', self.w, exp_term) * self.c[: i - 1]
                nominator = self.lambda0 + np.sum(sub_numerator / kappa_j_count)
                each_sum_term = numerator / nominator
                second_term += each_sum_term
        grad_beta_l = first_term - second_term
        return grad_beta_l

    def grad_tau_l(self, n, l_index):
        """
        calculate gradient for each tau
        :param l_index: update the l_index position beta
        :param n: event
        :return:
        """
        tau_l = self.tau[l_index]
        w_kl = self.w[l_index, :]
        beta_l = self.beta[l_index]

        # first term
        delta_T = self.timestamp_array[-1] - self.timestamp_array[: n]
        first_exp_term = np.exp(-delta_T / tau_l) - 1
        first_exp_term = beta_l * first_exp_term
        multiply = np.vectorize(np.multiply, signature='(k),()->(k)')
        first_numerator = multiply(w_kl, first_exp_term) * self.c[: n]
        kappa_i_count = np.count_nonzero(self.c[: n], axis=1).reshape(-1, 1)
        first_sub_term = first_numerator / kappa_i_count
        second_exp_term = np.exp(-delta_T / tau_l)
        second_numerator = beta_l * delta_T * second_exp_term
        second_numerator = multiply(w_kl, second_numerator) * self.c[: n]
        second_nominator = tau_l * kappa_i_count
        second_sub_term = second_numerator / second_nominator
        first_term = - np.sum(first_sub_term + second_sub_term)

        # second term
        second_term = 0
        for i in np.arange(1, n + 1):
            if i == 1:
                second_term += 1 / self.lambda0
            else:
                delta_ti_tj = self.timestamp_array[i - 1] - self.timestamp_array[: i - 1]
                numerator_exp_term = np.exp(-delta_ti_tj / tau_l)
                sub_numerator = beta_l * delta_ti_tj * numerator_exp_term
                sub_numerator = multiply(w_kl, sub_numerator) * self.c[: i - 1]
                sub_nominator = (tau_l ** 2) * np.count_nonzero(self.c[: i - 1], axis=1).reshape(-1, 1)
                numerator = np.sum(sub_numerator / sub_nominator)

                divide = np.vectorize(np.divide, signature='(),(l)->(l)')
                nominator_exp_term = np.exp(-divide(delta_ti_tj, self.tau))
                nominator_exp_term = self.beta * nominator_exp_term
                nominator_numerator = np.einsum('lk,tl->tk', self.w, nominator_exp_term) * self.c[: i - 1]
                nominator_nominator = np.count_nonzero(self.c[: i - 1], axis=1).reshape(-1, 1)
                nominator = np.sum(nominator_numerator / nominator_nominator)
                each_sum_term = numerator / self.lambda0 + nominator
                second_term += each_sum_term

        grad_tau_l = first_term - second_term
        return grad_tau_l

    def update_hyperparameter_grad_method(self, n, alpha_lambda, alpha_beta, alpha_tau, epsilon=0.001):
        """
        update hyperparameter using gradient descent algorithm
        :param epsilon:
        :param alpha_tau:
        :param alpha_beta:
        :param alpha_lambda:
        :param n: event
        :return:
        """
        i = 0
        while True:
            # calculate loss before updating
            loss_before = - self.log_hawkes_likelihood(n=n, lambda0=self.lambda0, beta=self.beta, tau=self.tau)
            # update parameters
            self.lambda0 = self.lambda0 - alpha_lambda * self.grad_lambda0(n=n)
            for l_index in np.arange(self.L):
                self.beta[l_index] = self.beta[l_index] - alpha_beta * self.grad_beta_l(n=n, l_index=l_index)
                self.tau[l_index] = self.tau[l_index] - alpha_tau * self.grad_tau_l(n=n, l_index=l_index)
            # calculate loss after updating
            loss_after = - self.log_hawkes_likelihood(n=n, lambda0=self.lambda0, beta=self.beta, tau=self.tau)
            delta_loss = loss_after - loss_before

            logging.info(f'[event {n}, iter {i}] lambda0: {self.lambda0}')
            logging.info(f'[event {n}, iter {i}] beta: {self.beta}')
            logging.info(f'[event {n}, iter {i}] tau: {self.tau}')
            logging.info(f'[event {n}, iter {i}] log likelihood before: {-loss_before}, '
                         f'log likelihood after: {-loss_after}, loss: {delta_loss}')

            if delta_loss <= epsilon:
                break
            i += 1

        print(f'[event {n}] updated lambda0: {self.lambda0}')
        print(f'[event {n}] updated beta: {self.beta}')
        print(f'[event {n}] updated tau: {self.tau}')

    # noinspection SpellCheckingInspection
    def update_log_particle_weight(self, old_particle_weight, n: int):
        """
        calculate and update log particle weight
        :param old_particle_weight: former event's particle weight
        :param n: The current sample number (sample order), should be greater than or equal to 1
        :return:
        """
        if n == 1:
            log_likelihood_timestamp = np.log(self.lambda0 * self.timestamp_array[0])
        else:
            divide = np.vectorize(np.divide, signature='(),(l)->(l)')
            integral_delta_tn_1_ti = self.timestamp_array[n - 2] - self.timestamp_array[: n]
            exp_tn_1_ti = np.exp(- divide(integral_delta_tn_1_ti, self.tau))
            integral_delta_tn_ti = self.timestamp_array[n - 1] - self.timestamp_array[: n]
            exp_tn_ti = np.exp(- divide(integral_delta_tn_ti, self.tau))
            exp_term = exp_tn_1_ti - exp_tn_ti  # (t, l)
            exp_term = self.beta * self.tau * exp_term
            exp_term = np.einsum('lk,tl->tk', self.w, exp_term) * self.c[: n]
            kappa_i_count = np.count_nonzero(self.c[: n], axis=1).reshape(-1, 1)
            c_i = np.argwhere(self.c[n - 1] != 0)[:, 0]  # nonzero element index for current event
            log_likelihood_timestamp = np.log(np.sum((exp_term / kappa_i_count)[:, c_i]))

        # # calculate lambda_prime
        # if n == 1:
        #     lambda_prime = self.lambda0 * self.timestamp_array[n - 1]
        # else:
        #     delta_tn_array = self.timestamp_array[n - 1] - self.timestamp_array[: n - 1]
        #     delta_tn_minus_1_array = self.timestamp_array[n - 2] - self.timestamp_array[: n - 1]
        #
        #     # calculate denominator
        #     kappa_history_count = np.count_nonzero(self.c[: n - 1], axis=1).reshape(-1, 1)
        #     kappa_n_nonzero_index = np.argwhere(self.c[n - 1] != 0)[:, 0]
        #
        #     # exp term
        #     dev_func = np.vectorize(np.divide, signature='(),(l)->(l)')
        #     exp_term = np.exp(-dev_func(delta_tn_minus_1_array, self.tau)) - np.exp(-dev_func(delta_tn_array, self.tau))
        #     multiply_func = np.vectorize(np.multiply)
        #     beta_tau_exp_term = multiply_func(self.beta * self.tau, exp_term)  # (T, L)
        #     nominator = np.einsum('lk,tl->tk', self.w, beta_tau_exp_term) * self.c[: n - 1]  # (T, K)
        #     fraction = nominator / kappa_history_count
        #     exp_term_res = np.sum(fraction[:, kappa_n_nonzero_index])  # kappa_n_nonzero_index
        #     lambda_prime = self.lambda0 * (self.timestamp_array[n - 1] - self.timestamp_array[n - 2]) + exp_term_res
        # # calculate log likelihood for timestamp
        # log_likelihood_timestamp = np.log(lambda_prime) - lambda_prime

        # log likelihood for text
        c_n = np.argwhere(self.c[n - 1] != 0)[:, 0]
        vn_avg = np.einsum('ij->i', self.v[:, c_n]) / np.count_nonzero(self.c[n - 1])
        Tn_all = transfer_multi_dist_result_to_vec(self.T_array)  # shape=(S, )
        Tn = Tn_all[n - 1]
        count_dict = Counter(Tn)
        log_likelihood_text = 0
        for k, v in count_dict.items():
            log_likelihood_text += v * np.log(vn_avg[k])

        if hasattr(self, 'FLAG'):
            print(f'Fixed particle\'s log_likelihood_timestamp: {log_likelihood_timestamp}')
            print(f'Fixed particle\'s log_likelihood_text: {log_likelihood_text}\n')
        elif hasattr(self, 'Parameter_FLAG'):
            print(f'Parameter Fixed particle\'s log_likelihood_timestamp: {log_likelihood_timestamp}')
            print(f'Parameter Fixed particle\'s log_likelihood_text: {log_likelihood_text}\n')
        else:
            print(f'log_likelihood_timestamp: {log_likelihood_timestamp}')
            print(f'log_likelihood_text: {log_likelihood_text}\n')

        # Calculate the updated log particle weight
        self.log_particle_weight = np.log(old_particle_weight) + log_likelihood_timestamp + log_likelihood_text


# noinspection PyPep8Naming,PyShadowingNames,SpellCheckingInspection
class StatusFixedParticle(Particle):

    def __init__(self, ibhp_instance, word_corpus, n_sample):
        """
        fix the parameters of this particle using the parameters that generated the simulation data
        """
        super(StatusFixedParticle, self).__init__(T_array=ibhp_instance.text, timestamp_array=ibhp_instance.timestamp_array,
                                                  word_dict=word_corpus)
        self.w = ibhp_instance.w
        self.v = ibhp_instance.v
        self.c = ibhp_instance.c
        self.event_num = n_sample
        self.FLAG = 'Fixed'


# noinspection SpellCheckingInspection
class ParameterFixedParticle(Particle):

    def __init__(self, ibhp_instance: IBHP, word_corpus):
        """
        fix w, v, hyperparameter of the particles
        """
        super(ParameterFixedParticle, self).__init__(T_array=ibhp_instance.text, timestamp_array=ibhp_instance.timestamp_array,
                                                     word_dict=word_corpus,
                                                     fix_params=True,
                                                     simulation_v=ibhp_instance.v,
                                                     simulation_w=ibhp_instance.w)
        self.lambda0 = ibhp_instance.lambda0
        self.beta = ibhp_instance.beta
        self.tau = ibhp_instance.tau
        self.Parameter_FLAG = 'Fixed'


# noinspection PyPep8Naming,SpellCheckingInspection,PyShadowingNames,DuplicatedCode
class Filtering:
    """
    This class controls weight updating, normalization, and resampling of all particles (in parallel)
    """

    def __init__(self, n_particle: int):
        """
        Generates particles and initializes particle weights

        :param n_particle: number of particles
        """
        self.n_sample = n_sample
        # self.n_particle = n_particle + 1  # this num is used in fixed particle
        self.n_particle = n_particle
        self.particle_list = [Particle(word_dict=word_dict,
                                       timestamp_array=ibhp.timestamp_array, T_array=ibhp.text,
                                       simulation_w=ibhp.w,
                                       simulation_v=ibhp.v,
                                       L=ibhp.L, fix_params=True) for
                              i in np.arange(self.n_particle)]
        # self.particle_list = [Fixed_Particle() for i in np.arange(n_particle)]  # this is used in fixed particle
        # self.particle_list = [ParameterFixedParticle() for i in
        #                       np.arange(self.n_particle)]  # Parameter_Fixed_Particle
        # fixed_paritcle = Fixed_Particle()  # this is used in fixed particle
        # self.particle_list.append(fixed_paritcle)  # add a fixed particle to particle list, used in fixed particle
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)

    def update_particle_weight_arr(self, index_particle_pair_list: List[Tuple[int, Particle]]):
        """
        Update particle weight list
        :param index_particle_pair_list:
        :return:
        """
        for idx, particle in index_particle_pair_list:
            self.particle_weight_arr[idx] = particle.log_particle_weight

    def normalize_particle_weight(self):
        """
        Normalize particle weights (map weights to 0-1 interval and sum to 1)
        :return:
        """
        self.particle_weight_arr = spe.softmax(self.particle_weight_arr)

    def resample_particles(self):
        """
        Resampling particles, resampling n_particle particles
        :return:
        """
        new_particle_list = []
        sorted_particle_index = np.argsort(
            self.particle_weight_arr)  # obtained the index of particle weights in ascending order
        sorted_particle_weight = self.particle_weight_arr[sorted_particle_index]  # Particle weights in ascending order
        # Construct the weight interval corresponding to the particle
        for i in np.arange(self.n_particle - 1, -1, -1):
            sorted_particle_weight[i] = np.sum(sorted_particle_weight[: i + 1])
        # resample n_particle particles
        for i in np.arange(self.n_particle):
            u = np.random.uniform(0, 1)
            nearest_elem_idx = np.argmin(np.abs(u - sorted_particle_weight))
            if u <= sorted_particle_weight[nearest_elem_idx]:
                print(f'resample following particle less: {sorted_particle_index[nearest_elem_idx] + 1}\n')
                new_particle = self.particle_list[sorted_particle_index[nearest_elem_idx]]
            else:
                while sorted_particle_weight[nearest_elem_idx] == sorted_particle_weight[nearest_elem_idx + 1]:
                    nearest_elem_idx += 1
                print(f'resample following particle more: {sorted_particle_index[nearest_elem_idx + 1] + 1}\n')
                new_particle = self.particle_list[sorted_particle_index[nearest_elem_idx + 1]]

            # important, if the same particle is sampled, they should be different objects.
            new_particle_list.append(deepcopy(new_particle))
        # Update the particle list and reset the particle weight
        self.particle_weight_arr = np.array([1 / self.n_particle] * self.n_particle)
        return new_particle_list

    def generate_first_event_status_for_each_particle(self, particle_idx_pair: Tuple[int, Particle]):
        """
        Generate the state corresponding to the first event for each particle
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        if hasattr(particle, 'FLAG'):
            particle.update_hyperparameter(n=1, method='average')
            hp_lh = particle.log_hawkes_likelihood(n=1, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event 1, paricle {particle_idx + 1}] Fixed Particle HP likelihood: {hp_lh}')
            logging.info(f'[event 1, paricle {particle_idx + 1}] Updating Fixed particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=1)
            return particle_idx, particle
        elif hasattr(particle, 'Parameter_FLAG'):
            logging.info(f'[event 1, paricle {particle_idx + 1}] Sampling Parameter Fixed particle status')
            particle.sample_first_particle_event_status(parameter_fixed=True)
            hp_lh = particle.log_hawkes_likelihood(n=1, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event 1, paricle {particle_idx + 1}] Parameter Fixed Particle HP likelihood: {hp_lh}')
            logging.info(f'[event 1, paricle {particle_idx + 1}] Updating Parameter Fixed particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=1)
            return particle_idx, particle
        else:
            logging.info(f'[event 1, paricle {particle_idx + 1}] Sampling particle status')
            particle.sample_first_particle_event_status()
            # Update hyperparameters and triggering kernels
            logging.info(f'[event 1, paricle {particle_idx + 1}] Updating hyperparameters')
            particle.update_hyperparameter(n=1)
            # Calculate and update the log particle weights
            hp_lh = particle.log_hawkes_likelihood(n=1, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event 1, paricle {particle_idx + 1}] Particle HP likelihood: {hp_lh}')
            logging.info(f'[event 1, paricle {particle_idx + 1}] Updating particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=1)
            return particle_idx, particle

    def generate_following_event_status_for_each_paritcle(self, n: int, particle_idx_pair: Tuple[int, Particle]):
        """
        Generate the state corresponding to the following event for each particle
        :param n:
        :param particle_idx_pair:
        :return:
        """
        particle_idx = particle_idx_pair[0]
        particle = particle_idx_pair[1]
        if hasattr(particle, 'FLAG'):
            particle.update_hyperparameter(n=n, method='average')
            hp_lh = particle.log_hawkes_likelihood(n=n, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event {n}, paricle {particle_idx + 1}] Fixed Particle HP likelihood: {hp_lh}')
            logging.info(f'[event {n}, particle {particle_idx + 1}] Updating particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=n)
            return particle_idx, particle
        elif hasattr(particle, 'Parameter_FLAG'):
            logging.info(f'[event {n}, paricle {particle_idx + 1}] Sampling Parameter Fixed particle status')
            particle.sample_particle_following_event_status(n)
            hp_lh = particle.log_hawkes_likelihood(n=n, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event {n}, paricle {particle_idx + 1}] Parameter Fixed Particle HP likelihood: {hp_lh}')
            logging.info(f'[event {n}, paricle {particle_idx + 1}] Updating Parameter Fixed particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=n)
            return particle_idx, particle
        else:
            logging.info(f'[event {n}, particle {particle_idx + 1}] Sampling particle status')
            particle.sample_particle_following_event_status(n)
            logging.info(f'[event {n}, particle {particle_idx + 1}] Updating hyperparameters')
            particle.update_hyperparameter(n=n)
            hp_lh = particle.log_hawkes_likelihood(n=n, lambda0=particle.lambda0, beta=particle.beta, tau=particle.tau)
            logging.info(f'[event {n}, paricle {particle_idx + 1}] Particle HP likelihood: {hp_lh}')
            logging.info(f'[event {n}, particle {particle_idx + 1}] Updating particle weight')
            particle.update_log_particle_weight(old_particle_weight=self.particle_weight_arr[particle_idx], n=n)
            return particle_idx, particle


if __name__ == '__main__':
    # ------------------------------ generate simulation data ------------------------------

    n_sample = 200
    n_particle = 10
    # noinspection SpellCheckingInspection
    ibhp = IBHP(n_sample=n_sample, random_seed=10)
    ibhp.generate_data()
    true_intensity_array = ibhp.lambda_tn_array
    logging.info(f'\n{"-" * 40} Observational data generated {"-" * 40}\n')
    logging.info(f'Timestamp: {ibhp.timestamp_array}\n')
    logging.info(f'Text: {transfer_multi_dist_result_to_vec(ibhp.text)}\n')
    word_dict = np.arange(1000)

    # ---------------------------- save simulation data ----------------------------
    # noinspection SpellCheckingInspection
    SAVE_FLAG = False
    TIME = datetime.strftime(datetime.now(), '%Y_%m_%d_%H_%M_%S')
    SAVE_DIR = f'./model_result/model_result_{TIME}'

    if SAVE_FLAG:
        # save the model result
        if not os.path.exists('./model_result'):
            os.mkdir('./model_result')
        if not os.path.exists(f'{SAVE_DIR}'):
            os.mkdir(f'{SAVE_DIR}')
        # clean old model result
        else:
            os.system(f'rm -r {SAVE_DIR}/*')

        # save true value
        np.save(f'{SAVE_DIR}/true_intensity_array.npy', true_intensity_array)
        np.save(f'{SAVE_DIR}/time_stamp_array.npy', ibhp.timestamp_array)
        np.save(f'{SAVE_DIR}/text_array.npy', transfer_multi_dist_result_to_vec(ibhp.text))
        np.save(f'{SAVE_DIR}/lambda_k_mat.npy', ibhp.lambda_k_array_mat)

    # -------------------------------- filtering --------------------------------
    # noinspection PyBroadException,SpellCheckingInspection
    logging.info(f'\n{"-" * 40}  Start particle filter parameter estimation {"-" * 40}\n')
    pf = Filtering(n_particle=n_particle)
    particle_index_pair_list_before_update = [(idx, particle) for idx, particle in enumerate(pf.particle_list)]

    # event 1 status
    # pool_event_1 = Pool(cpu_count())
    # particle_index_pair_list_after_update = list(pool_event_1.map(pf.generate_first_event_status_for_each_particle,
    #                                                               particle_index_pair_list_before_update))
    # pool_event_1.close()
    # pool_event_1.join()

    particle_index_pair_list_after_update = []
    for pair in particle_index_pair_list_before_update:
        updated_pair = pf.generate_first_event_status_for_each_particle(pair)
        particle_index_pair_list_after_update.append(updated_pair)

    # important, update particle in self.particle_list, which will be used in particle resampling
    for idx, particle in particle_index_pair_list_after_update:
        pf.particle_list[idx] = deepcopy(particle)

    # update particle_list
    pf.update_particle_weight_arr(particle_index_pair_list_after_update)
    pf.normalize_particle_weight()
    logging.info(f'[event 1] Normalized particle weight: \n{pf.particle_weight_arr}')
    RESAMPLING_FLAG = False

    # ------------------------------- output -------------------------------
    # noinspection DuplicatedCode
    p_weight_arr = pf.particle_weight_arr
    # parameters weighted average, use the best top_n particle
    # noinspection DuplicatedCode
    lam_0_arr = np.array(
        [particle.lambda0 for idx, particle in particle_index_pair_list_after_update])
    lam_0 = np.average(lam_0_arr, weights=p_weight_arr)
    beta_arr = np.array(
        [particle.beta.reshape(-1) for idx, particle in particle_index_pair_list_after_update])
    beta = np.average(beta_arr, axis=0, weights=p_weight_arr)
    tau_arr = np.array(
        [particle.tau.reshape(-1) for idx, particle in particle_index_pair_list_after_update])
    tau = np.average(tau_arr, axis=0, weights=p_weight_arr)
    logging.info(
        f'Average weighted value of three parameters: \n lambda_0: {lam_0}\n beta: {beta}\n tau: {tau}\n')
    pred_lambda0_array = np.array([lam_0])
    pred_beta_array = beta.reshape(1, -1)
    pred_tau_array = tau.reshape(1, -1)

    # noinspection DuplicatedCode
    # save result
    # params
    if SAVE_FLAG:
        np.save(f'{SAVE_DIR}/pred_lambda_0.npy', pred_lambda0_array)
        np.save(f'{SAVE_DIR}/pred_beta.npy', pred_beta_array)
        np.save(f'{SAVE_DIR}/pred_tau.npy', pred_tau_array)

        # particle weight
        np.save(f'{SAVE_DIR}/particle_weight.npy', pf.particle_weight_arr)
    # hidden variables in specific particle
    if SAVE_FLAG:
        for idx, particle in particle_index_pair_list_after_update:
            if not os.path.exists(f'{SAVE_DIR}/particle-{idx}'):
                os.mkdir(f'{SAVE_DIR}/particle-{idx}')
            np.save(f'{SAVE_DIR}/particle-{idx}/pred_lambda_tn.npy', particle.lambda_tn_array)
            np.save(f'{SAVE_DIR}/particle-{idx}/pred_lambda_k_mat.npy', particle.lambda_k_array_mat)
    # resampling
    N_eff = 1 / np.sum(np.square(pf.particle_weight_arr))
    if N_eff < 0.8 * pf.n_particle:
        logging.info(f'[event 1] Resampling particles')
        new_particle_list = pf.resample_particles()
        particle_index_pair_list_before_update = [(idx, particle) for idx, particle in enumerate(new_particle_list)]
        RESAMPLING_FLAG = True

    # event 2~n status
    # noinspection SpellCheckingInspection
    for n in np.arange(2, n_sample + 1):
        if not RESAMPLING_FLAG:
            particle_index_pair_list_before_update = deepcopy(particle_index_pair_list_after_update)

        # process pool for event n
        # pool_event_n = Pool(cpu_count())
        # particle_index_pair_list_after_update = list(
        #     pool_event_n.map(partial(pf.generate_following_event_status_for_each_paritcle, n),
        #                      particle_index_pair_list_before_update))
        # pool_event_n.close()
        # pool_event_n.join()

        # noinspection DuplicatedCode
        particle_index_pair_list_after_update = []
        for pair in particle_index_pair_list_before_update:
            updated_pair = pf.generate_following_event_status_for_each_paritcle(n, pair)
            particle_index_pair_list_after_update.append(updated_pair)

        # update particle state for each particle in particle_list
        for idx, particle in particle_index_pair_list_after_update:
            pf.particle_list[idx] = deepcopy(particle)

        pf.update_particle_weight_arr(particle_index_pair_list_after_update)
        pf.normalize_particle_weight()
        RESAMPLING_FLAG = False
        logging.info(f'[event {n}] Normalized particle weight: {pf.particle_weight_arr}')
        # ------------------------------- output -------------------------------
        # particle weight
        # noinspection DuplicatedCode
        p_weight_arr = pf.particle_weight_arr

        # Hyperparameter weighted average
        # noinspection DuplicatedCode
        lam_0_arr = np.array(
            [particle.lambda0 for idx, particle in particle_index_pair_list_after_update])
        lam_0 = np.average(lam_0_arr, weights=p_weight_arr)
        beta_arr = np.array([particle.beta.reshape(-1) for idx, particle in particle_index_pair_list_after_update])
        beta = np.average(beta_arr, axis=0, weights=p_weight_arr)
        tau_arr = np.array(
            [particle.tau.reshape(-1) for idx, particle in particle_index_pair_list_after_update])
        tau = np.average(tau_arr, axis=0, weights=p_weight_arr)
        logging.info(
            f'Average weighted value of three parameters: \n lambda_0: {lam_0}\n beta: {beta}\n tau: {tau}\n')
        pred_lambda0_array = np.append(pred_lambda0_array, lam_0)
        pred_beta_array = np.vstack((pred_beta_array, beta))
        pred_tau_array = np.vstack((pred_tau_array, tau))
        # save result
        # params
        # noinspection DuplicatedCode
        if SAVE_FLAG:
            np.save(f'{SAVE_DIR}/pred_lambda_0.npy', pred_lambda0_array)
            np.save(f'{SAVE_DIR}/pred_beta.npy', pred_beta_array)
            np.save(f'{SAVE_DIR}/pred_tau.npy', pred_tau_array)

            # particle weight
            np.save(f'{SAVE_DIR}/particle_weight.npy', pf.particle_weight_arr)
        # hidden variables in specific particle
        if SAVE_FLAG:
            for idx, particle in particle_index_pair_list_after_update:
                if not os.path.exists(f'{SAVE_DIR}/particle-{idx}'):
                    os.mkdir(f'{SAVE_DIR}/particle-{idx}')
                np.save(f'{SAVE_DIR}/particle-{idx}/pred_lambda_tn.npy', particle.lambda_tn_array)
                np.save(f'{SAVE_DIR}/particle-{idx}/pred_lambda_k_mat.npy',
                        particle.lambda_k_array_mat)
        if n == n_sample:
            break
        # resampling
        N_eff = 1 / np.sum(np.square(pf.particle_weight_arr))
        if N_eff < 0.8 * pf.n_particle:
            logging.info(f'[event {n}] Resampling particles')
            new_particle_list = pf.resample_particles()
            particle_index_pair_list_before_update = [(idx, particle) for idx, particle in enumerate(new_particle_list)]
            RESAMPLING_FLAG = True
