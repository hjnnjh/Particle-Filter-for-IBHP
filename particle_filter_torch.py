#!/usr/bin/env python
# -*- encoding: utf-8 -*-
"""
@File    :   particle_filter_torch.py
@Time    :   2022/4/2 17:24
@Author  :   Jinnan Huang 
@Contact :   jinnan_huang@stu.xjtu.edu.cn
@Desc    :   None
"""
import logging
from functools import partial
from typing import Tuple

import numpy as np
import torch
import torch.distributions as dist
from functorch import vmap

from IBHP_simulation import IBHP

# ------------------------------ global vars ------------------------------

DEVICE0 = torch.device('cuda:0')
TENSOR = torch.Tensor


# ------------------------------ utils ------------------------------

def transfer_multi_dist_result_to_vec(text):
    """
    transform the data matrix generated by multinomial distribution to a concrete word matrix
    :param text: text vector
    :return:
    """

    # noinspection DuplicatedCode
    def transfer_occurrence_data(idx: list, data: list):
        res = []
        res.extend([[idx[i]] * data[i] for i in range(len(idx))])
        res = [i for k in res for i in k]
        return res

    word_index = torch.argwhere(text != 0)
    index_list = [word_index[word_index[:, 0] == n][:, 1].tolist() for n in torch.unique(word_index[:, 0])]
    word_occurrence_list = [text[i, idx].tolist() for i, idx in enumerate(index_list)]
    word_corpus_mat = torch.tensor([transfer_occurrence_data(index_list[i], word_occurrence_list[i])
                                    for i in range(len(index_list))]).to(DEVICE0)
    return word_corpus_mat


# noinspection SpellCheckingInspection,PyTypeChecker,DuplicatedCode
class Particle(IBHP):
    """
        This class implements all the steps of single particle sampling, hyperparameter updating and particle weight
        calculation.
    """
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S",
                        level=logging.INFO)

    def __init__(self, word_corpus: TENSOR, timestamp_tensor: TENSOR, text_tensor: TENSOR,
                 sum_kernel_num: int = 3,
                 simulation_w: TENSOR = None, simulation_v: TENSOR = None, fix_w_v: bool = False,
                 random_seed: int = None):
        """
        :param word_corpus: all unique words
        :param timestamp_tensor: timestamp vector of real events
        :param text_tensor: text vector of real events
        :param sum_kernel_num: number of sum kernels
        :param simulation_w: w vector of simulation data
        :param simulation_v: v vector of simulation data
        :param fix_w_v: whether fix particle status
        """
        super(Particle, self).__init__()
        self.random_seed = random_seed
        self.sum_kernel_num = torch.tensor(sum_kernel_num).to(DEVICE0)
        self.text_tensor = text_tensor.to(DEVICE0)
        self.timestamp_tensor = timestamp_tensor.to(DEVICE0)
        self.word_corpus = word_corpus.to(DEVICE0)
        self.word_num = word_corpus.shape[0]
        self.log_particle_weight = None
        self.lambda_k_tensor = None
        self.lambda_k_tensor_mat = None
        self.lambda_tn_tensor = None
        self.fix_w_v = fix_w_v
        if self.fix_w_v:
            self.simulation_v = simulation_v.to(DEVICE0)
            self.simulation_w = simulation_w.to(DEVICE0)
            assert self.simulation_w.shape[1] == self.simulation_v.shape[1]
            self.simulation_factor_num = self.simulation_w.shape[1]
        if self.random_seed:
            np.random.seed(self.random_seed)
            torch.manual_seed(self.random_seed)
        # torch.set_default_tensor_type(torch.FloatTensor)  # to use as less GPU memory as possible

    @staticmethod
    def exp_kernel(delta_t: TENSOR, beta: TENSOR, tau: TENSOR):
        """
        exp kernel
        :param delta_t:
        :param beta:
        :param tau:
        :return:
        """
        return beta * torch.exp(-delta_t / tau)

    def calculate_lambda_k(self, n):
        """
        compute lambda_k tensor for each event
        :param n:
        :return:
        """
        if n == 1:
            self.lambda_k_tensor = self.w.T @ self.beta
        else:
            delta_t_tensor = self.timestamp_tensor[n - 1] - self.timestamp_tensor[: n]
            exp_kernel_vfunc = vmap(self.exp_kernel, in_dims=(0, None, None))
            exp_kernel_mat = exp_kernel_vfunc(delta_t_tensor, self.beta, self.tau)
            kappa_history = torch.einsum('lk,tl->tk', self.w, exp_kernel_mat) * self.c
            kappa_history_count = torch.count_nonzero(self.c, dim=1).reshape(-1, 1)
            self.lambda_k_tensor = torch.sum(kappa_history / kappa_history_count, dim=0)

    def collect_factor_intensity(self, n):
        """
        Collect intensity of all factors generated during the simulation
        :param n: event
        :return:
        """
        if n == 1:
            self.lambda_k_tensor_mat = self.lambda_k_tensor.reshape(1, -1)
        else:
            zero_num = self.lambda_k_tensor.shape[0] - self.lambda_k_tensor_mat[-1].shape[0]
            if zero_num:
                self.lambda_k_tensor_mat = torch.hstack(
                    (self.lambda_k_tensor_mat, torch.zeros((self.lambda_k_tensor_mat.shape[0], zero_num)).to(DEVICE0)))
            self.lambda_k_tensor_mat = torch.vstack((self.lambda_k_tensor_mat, self.lambda_k_tensor))

    def sample_particle_first_event_status(self, particle_idx: int, lambda0: TENSOR, beta: TENSOR, tau: TENSOR):
        """
        generate the particle state corresponding to the first Event
        :param particle_idx:
        :param tau:
        :param beta:
        :param lambda0:
        :return:
        """
        if self.fix_w_v:
            pass
        else:
            self.lambda0 = lambda0.to(DEVICE0)
            self.beta = beta.to(DEVICE0)
            self.tau = tau.to(DEVICE0)
        self.w_0 = torch.tensor([1 / self.sum_kernel_num] * self.sum_kernel_num).to(DEVICE0)
        self.v_0 = torch.tensor([1 / self.word_num] * self.word_num).to(DEVICE0)

        # generate init K
        self.K = torch.poisson(self.lambda0)
        self.K = self.K.int()
        while self.K == 0:
            self.K = torch.poisson(self.lambda0)
            self.K = self.K.int()
        self.c = torch.ones((1, self.K)).to(DEVICE0)

        # generate w and v
        if self.fix_w_v:  # fix w and v
            if self.simulation_factor_num >= self.K:
                self.w = self.simulation_w[:, : self.K]
                self.v = self.simulation_v[:, : self.K]
            else:
                w = dist.Dirichlet(self.w_0).sample((self.K - self.simulation_factor_num,)).T
                self.w = torch.hstack((self.simulation_w[:, : self.K], w))
                v = dist.Dirichlet(self.v_0).sample((self.K - self.simulation_factor_num,)).T
                self.v = torch.hstack((self.simulation_v[:, : self.K], v))
        else:
            self.w = dist.Dirichlet(self.w_0).sample((self.K,)).T
            self.v = dist.Dirichlet(self.v_0).sample((self.K,)).T

        # compute lambda_k_1
        self.calculate_lambda_k(1)
        c_1 = torch.argwhere(self.c[-1] != 0)[:, 0]
        self.lambda_tn_tensor = torch.sum(self.lambda_k_tensor[c_1].reshape(1, -1), dim=1)
        logging.info(f'[event 1, particle {particle_idx + 1}] factor occurence(c) shape: {self.c.shape}')

        # self.collect_factor_intensity(1)
        # logging.info(f'[event 1, particle {particle_idx + 1}] lambda k shape: {self.lambda_k_tensor_mat.shape}\n')

    def sample_particle_following_event_status(self, n: int, particle_idx: int):
        p = self.lambda_k_tensor / ((self.lambda0 / self.K) + self.lambda_k_tensor)
        c_old = torch.bernoulli(p)
        k_plus = torch.poisson(self.lambda0 / (self.lambda0 + torch.sum(self.lambda_k_tensor)))
        k_plus = k_plus.int()
        # When K+ is equal to 0, check whether c_old is all 0, and if it is all 0, regenerate c_old
        if not k_plus:
            while torch.all(c_old == 0):
                c_old = torch.bernoulli(p)
        # update K
        self.K += k_plus
        if k_plus:
            c_new = torch.ones(k_plus).to(DEVICE0)
            c = torch.hstack((c_old, c_new))
            self.c = torch.hstack(
                (self.c, torch.zeros((self.c.shape[0], k_plus)).to(DEVICE0)))  # fill existing c matrix
            self.c = torch.vstack((self.c, c))

            # fix w and v
            if self.fix_w_v:
                if self.simulation_factor_num >= self.K:
                    self.w = self.simulation_w[:, : self.K]
                    self.v = self.simulation_v[:, : self.K]
                else:
                    w = dist.Dirichlet(self.w_0).sample((self.K - self.simulation_factor_num,)).T
                    self.w = torch.hstack((self.simulation_w[:, : self.K], w))
                    v = dist.Dirichlet(self.v_0).sample((self.K - self.simulation_factor_num,)).T
                    self.v = torch.hstack((self.simulation_v[:, : self.K], v))
            else:
                w_new = dist.Dirichlet(self.w_0).sample((k_plus,)).T
                self.w = torch.hstack((self.w, w_new))
                v_new = dist.Dirichlet(self.v_0).sample((k_plus,)).T
                self.v = torch.hstack((self.v, v_new))
        else:
            self.c = torch.vstack((self.c, c_old))

        # compute lambda_tn_k, lambda_tn
        self.calculate_lambda_k(n)
        c_n = torch.argwhere(self.c[-1] != 0)[:, 0]
        self.lambda_tn_tensor = torch.hstack((self.lambda_tn_tensor, torch.sum(self.lambda_k_tensor[c_n])))
        logging.info(f'[event {n}, particle {particle_idx + 1}] factor occurence(c) shape: {self.c.shape}')
        # self.collect_factor_intensity(n)
        # logging.info(f'[event {n}, particle {particle_idx + 1}] lambda k shape: {self.lambda_k_tensor_mat.shape}\n')

    @staticmethod
    def compute_delta_t(all_t, t, t_index):
        """
        this func in util func of `self.compute_sum_term_delta_t`
        :param all_t:
        :param t:
        :param t_index:
        :return:
        """
        return t - all_t[: t_index]

    @staticmethod
    def fill_delta_t_tensor(delta_t_tensor_tuple: Tuple):
        """
        fill delta_t tensor with tensor(0.)
        :param delta_t_tensor_tuple:
        :return:
        """
        max_length = delta_t_tensor_tuple[-1].shape[0]
        filled_tuple = tuple(
            map(lambda t: torch.hstack((t, torch.zeros(max_length - t.shape[0]).to(DEVICE0))) if max_length - t.shape[
                0] > 0 else t,
                delta_t_tensor_tuple))
        return filled_tuple

    def compute_sum_term_delta_t(self, n):
        """
        calculate delta_t tensor in sum term of HP likelihood
        :param n:
        :return:
        """
        delta_ti_1_tj = tuple(map(partial(self.compute_delta_t, self.timestamp_tensor[: n]),
                                  self.timestamp_tensor[: n - 1],
                                  self.timestamp_tensor[: n - 1].argwhere().squeeze(1) + 1))
        delta_ti_1_tj = self.fill_delta_t_tensor(delta_ti_1_tj)
        delta_ti_1_tj = torch.stack(delta_ti_1_tj, dim=0)
        delta_ti_tj = tuple(map(partial(self.compute_delta_t, self.timestamp_tensor[: n]),
                                self.timestamp_tensor[1: n],
                                self.timestamp_tensor[1: n].argwhere().squeeze(1) + 2))
        delta_ti_tj = self.fill_delta_t_tensor(delta_ti_tj)
        delta_ti_tj = torch.stack(delta_ti_tj, dim=0)
        return delta_ti_1_tj, delta_ti_tj

    def hawkes_likelihood_integral_sum_term_for_loop(self, n, beta, tau):
        """
        :param n:
        :param beta:
        :param tau:
        :return:
        """
        divide_vfunc = vmap(torch.divide, in_dims=(0, None))
        sum_term = torch.tensor(0.).to(DEVICE0)
        for i in torch.arange(2, n + 1):
            integral_delta_ti_1_tj = self.timestamp_tensor[i - 2] - self.timestamp_tensor[: i - 1]
            exp_ti_1_tj = torch.exp(-divide_vfunc(integral_delta_ti_1_tj, tau))
            integral_delta_ti_tj = self.timestamp_tensor[i - 1] - self.timestamp_tensor[: i - 1]
            exp_ti_tj = torch.exp(-divide_vfunc(integral_delta_ti_tj, tau))
            exp_term = exp_ti_tj - exp_ti_1_tj
            exp_term = beta * tau * exp_term
            # exp_term_einsum = torch.einsum('lk,tl->tk', self.w, exp_term) * self.c[: i - 1]
            kappa_j_count = torch.count_nonzero(self.c[: i - 1], dim=1).reshape(-1, 1)
            # c_i = torch.argwhere(self.c[i - 1] != 0)[:, 0]
            sum_j_integral = torch.sum((exp_term / kappa_j_count))
            sum_term.add_(sum_j_integral)
        print(sum_term)
        return sum_term

    def hawkes_likelihood_integral_sum_term(self, n: int, beta: TENSOR, tau: TENSOR, delta_ti_1_tj: TENSOR,
                                            delta_ti_tj: TENSOR):
        """
        compute each inteagral term in log hawkes likelihood
        :param delta_ti_1_tj:
        :param delta_ti_tj:
        :param n:
        :param tau:
        :param beta:
        :return:
        """
        divide_vfunc = vmap(vmap(torch.divide, in_dims=(0, None)), in_dims=(0, None))
        exp_ti_1_tj = torch.exp(-divide_vfunc(delta_ti_1_tj, tau))
        exp_ti_tj = torch.exp(-divide_vfunc(delta_ti_tj, tau))
        exp_term = exp_ti_tj - exp_ti_1_tj
        exp_term.mul_(beta * tau)
        # exp_term = beta * tau * exp_term
        exp_term_einsum = torch.einsum('lk,ntl->ntk', self.w, exp_term) * self.c[: n]  # (n-1, n, K)
        exp_term_einsum.mul_(self.c[1: n].unsqueeze(1))
        kappa_j_count = torch.count_nonzero(self.c[: n], dim=1).reshape(-1, 1)
        sum_integral_candidate = (exp_term_einsum / kappa_j_count).reshape((n - 1) * n, -1)
        sum_term = torch.sum(sum_integral_candidate.index_select(0, exp_term_2d_index))
        return sum_term

    def hawkes_likelihood_prod_term(self, n, beta: TENSOR, tau: TENSOR):
        """
        compute each prod term in log hawkes likelihood
        :param n:
        :param beta:
        :param tau:
        :return:
        """
        divide_vfunc = vmap(vmap(torch.divide, in_dims=(0, None)), in_dims=(0, None))
        compute_delta_t_vfunc = vmap(lambda t, all_t: t - all_t, in_dims=(0, None))

        delta_ti_tj = compute_delta_t_vfunc(self.timestamp_tensor[: n], self.timestamp_tensor[: n])  # (n, n, L)
        exp_term = torch.exp(-divide_vfunc(delta_ti_tj, tau))
        valid_exp_term_row_index = torch.argwhere(exp_term <= 1)[:, :2].unique(
            dim=0)
        exp_term_2d_index = valid_exp_term_row_index[:, 0] * n + valid_exp_term_row_index[:, 1]
        exp_term.mul_(beta)
        exp_term_einsum = torch.einsum('lk,ntl->ntk', self.w, exp_term) * self.c[: n]  # (n, n, K)
        exp_term_einsum.mul_(self.c[: n].unsqueeze(1))
        kappa_j_count = torch.count_nonzero(self.c[: n], dim=1).reshape(-1, 1)
        sum_prod_candidate = (exp_term_einsum / kappa_j_count).reshape(n * n, -1)
        sum_prod_candidate = sum_prod_candidate.index_select(0, exp_term_2d_index)
        log_prod_term = torch.sum(torch.log(sum_prod_candidate))
        return log_prod_term

    def log_hawkes_likelihood(self, n: int, sum_delta_ti_1_tj: TENSOR, sum_delta_ti_tj: TENSOR,
                              lambda0: TENSOR, beta: TENSOR, tau: TENSOR):
        """
        log hawkes likelihood for IBHP
        :param sum_delta_ti_tj:
        :param sum_delta_ti_1_tj:
        :param n:
        :param lambda0:
        :param beta:
        :param tau:
        :return:
        """
        if n == 1:
            log_integral_term = - lambda0 * self.timestamp_tensor[0]
        else:
            sum_delta_ti_1_tj, sum_delta_ti_tj = self.compute_sum_term_delta_t(n=n)
            sum_term = self.hawkes_likelihood_integral_sum_term(n=n, beta=beta, tau=tau,
                                                                delta_ti_1_tj=sum_delta_ti_1_tj,
                                                                delta_ti_tj=sum_delta_ti_tj)
            log_integral_term = sum_term
            return log_integral_term
